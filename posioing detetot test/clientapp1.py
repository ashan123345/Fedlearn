{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bbbdfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 21:31:08,901 - FraminghamClient - INFO - Using device: cpu\n",
      "2025-05-23 21:31:08,907 - FraminghamClient - INFO - Loaded framingham_part1.csv with shape (1060, 16)\n",
      "2025-05-23 21:31:08,911 - FraminghamClient - INFO - Class distribution: {0: 884, 1: 176}\n",
      "2025-05-23 21:31:08,917 - FraminghamClient - INFO - Created dataloader with 1060 samples and 15 features\n",
      "2025-05-23 21:31:08,920 - FraminghamClient - INFO - Model initialized with input size: 15\n",
      "2025-05-23 21:31:08,921 - FraminghamClient - INFO - Initialized client 0 with device: cpu\n",
      "2025-05-23 21:31:08,923 - FraminghamClient - INFO - LR Schedule: linear_increase\n",
      "2025-05-23 21:31:08,924 - FraminghamClient - INFO - Initial LR: 0.0005\n",
      "2025-05-23 21:31:08,925 - FraminghamClient - INFO - Max LR: 0.005\n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-05-23 21:31:08,927 - flwr - WARNING - DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-05-23 21:31:08,931 - flwr - DEBUG - Opened insecure gRPC connection (no certificates were passed)\n",
      "2025-05-23 21:31:08,954 - flwr - DEBUG - ChannelConnectivity.IDLE\n",
      "2025-05-23 21:31:08,960 - flwr - DEBUG - ChannelConnectivity.CONNECTING\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Enhanced Training Client with Learning Rate Scheduling...\n",
      "\n",
      "===== Enhanced FL Client 0 (Dynamic Learning Rate) =====\n",
      "Server:           localhost:8080\n",
      "Data file:        framingham_part1.csv\n",
      "Local epochs:     3\n",
      "LR Schedule:      linear_increase\n",
      "Initial LR:       0.0005\n",
      "Max LR:           0.005\n",
      "LR Factor:        1.3\n",
      "Proximal mu:      0.01\n",
      "Device:           cpu\n",
      "=================================================================\n",
      "\n",
      "Connecting to server...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 21:31:11,271 - flwr - DEBUG - ChannelConnectivity.TRANSIENT_FAILURE\n",
      "2025-05-23 21:31:11,487 - flwr - DEBUG - gRPC channel closed\n"
     ]
    },
    {
     "ename": "_MultiThreadedRendezvous",
     "evalue": "<_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:8080: ConnectEx: Connection refused (No connection could be made because the target machine actively refused it.\r\n -- 10061)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:8080: ConnectEx: Connection refused (No connection could be made because the target machine actively refused it.\\r\\n -- 10061)\", grpc_status:14, created_time:\"2025-05-23T16:01:11.2864343+00:00\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 356\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# You can modify these parameters\u001b[39;00m\n\u001b[0;32m    349\u001b[0m CLIENT_CONFIG\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_schedule\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear_increase\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Change to: \"exponential_increase\", \"cosine\", \"step\"\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_learning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0005\u001b[39m,  \u001b[38;5;66;03m# Start slower\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_learning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.005\u001b[39m,       \u001b[38;5;66;03m# Cap at reasonable level\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_factor\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.3\u001b[39m                  \u001b[38;5;66;03m# Growth factor for exponential\u001b[39;00m\n\u001b[0;32m    354\u001b[0m })\n\u001b[1;32m--> 356\u001b[0m \u001b[43mstart_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 343\u001b[0m, in \u001b[0;36mstart_client\u001b[1;34m(client_id, server_address)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m65\u001b[39m)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConnecting to server...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 343\u001b[0m \u001b[43mfl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLIENT_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserver_address\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Rreserch work\\fedenvioremnt\\.venv\\lib\\site-packages\\flwr\\client\\app.py:201\u001b[0m, in \u001b[0;36mstart_client\u001b[1;34m(server_address, client_fn, client, grpc_max_message_length, root_certificates, insecure, transport, authentication_keys, max_retries, max_wait_time)\u001b[0m\n\u001b[0;32m    198\u001b[0m warn_deprecated_feature(name\u001b[38;5;241m=\u001b[39mmsg)\n\u001b[0;32m    200\u001b[0m event(EventType\u001b[38;5;241m.\u001b[39mSTART_CLIENT_ENTER)\n\u001b[1;32m--> 201\u001b[0m \u001b[43mstart_client_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_client_app_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrpc_max_message_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrpc_max_message_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_certificates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_certificates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43minsecure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minsecure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauthentication_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauthentication_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m event(EventType\u001b[38;5;241m.\u001b[39mSTART_CLIENT_LEAVE)\n",
      "File \u001b[1;32md:\\Rreserch work\\fedenvioremnt\\.venv\\lib\\site-packages\\flwr\\client\\app.py:438\u001b[0m, in \u001b[0;36mstart_client_internal\u001b[1;34m(server_address, node_config, load_client_app_fn, client_fn, client, grpc_max_message_length, root_certificates, insecure, transport, authentication_keys, max_retries, max_wait_time, flwr_path, isolation, clientappio_api_address)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;66;03m# Receive\u001b[39;00m\n\u001b[1;32m--> 438\u001b[0m         message \u001b[38;5;241m=\u001b[39m \u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Wait for 3s before asking again\u001b[39;00m\n",
      "File \u001b[1;32md:\\Rreserch work\\fedenvioremnt\\.venv\\lib\\site-packages\\flwr\\client\\grpc_client\\connection.py:140\u001b[0m, in \u001b[0;36mgrpc_connection.<locals>.receive\u001b[1;34m()\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreceive\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# Receive ServerMessage proto\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mserver_message_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# ServerMessage proto --> *Ins --> RecordSet\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     field \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mWhichOneof(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Rreserch work\\fedenvioremnt\\.venv\\lib\\site-packages\\grpc\\_channel.py:543\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Rreserch work\\fedenvioremnt\\.venv\\lib\\site-packages\\grpc\\_channel.py:969\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:8080: ConnectEx: Connection refused (No connection could be made because the target machine actively refused it.\r\n -- 10061)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:8080: ConnectEx: Connection refused (No connection could be made because the target machine actively refused it.\\r\\n -- 10061)\", grpc_status:14, created_time:\"2025-05-23T16:01:11.2864343+00:00\"}\"\n>"
     ]
    }
   ],
   "source": [
    "# Training Client with Learning Rate Schedule\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(\"FraminghamClient\")\n",
    "\n",
    "# Client configuration with learning rate scheduling\n",
    "CLIENT_CONFIG = {\n",
    "    \"local_epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"initial_learning_rate\": 0.001,  # Starting learning rate\n",
    "    \"lr_schedule\": \"linear_increase\",  # Options: \"linear_increase\", \"exponential_increase\", \"cosine\", \"step\"\n",
    "    \"lr_factor\": 1.2,  # Factor to increase LR each round\n",
    "    \"max_learning_rate\": 0.01,  # Maximum learning rate cap\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"dropout_rate\": 0.3,\n",
    "    \"server_address\": \"localhost:8080\",\n",
    "    \"proximal_mu\": 0.01\n",
    "}\n",
    "\n",
    "# Model for Framingham Heart Study data\n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CLIENT_CONFIG[\"dropout_rate\"]),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(CLIENT_CONFIG[\"dropout_rate\"]),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Learning Rate Schedulers\n",
    "class LearningRateScheduler:\n",
    "    def __init__(self, initial_lr, schedule_type=\"linear_increase\", factor=1.2, max_lr=0.01):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.schedule_type = schedule_type\n",
    "        self.factor = factor\n",
    "        self.max_lr = max_lr\n",
    "    \n",
    "    def get_lr(self, round_num):\n",
    "        \"\"\"Get learning rate for specific round\"\"\"\n",
    "        if self.schedule_type == \"linear_increase\":\n",
    "            # Linear increase: lr = initial_lr + (round * step_size)\n",
    "            step_size = (self.max_lr - self.initial_lr) / 10  # Assuming 10 rounds\n",
    "            lr = self.initial_lr + (round_num * step_size)\n",
    "            \n",
    "        elif self.schedule_type == \"exponential_increase\":\n",
    "            # Exponential increase: lr = initial_lr * (factor ^ round)\n",
    "            lr = self.initial_lr * (self.factor ** round_num)\n",
    "            \n",
    "        elif self.schedule_type == \"cosine\":\n",
    "            # Cosine annealing (increases first half, decreases second half)\n",
    "            # For 10 rounds, peak at round 5\n",
    "            lr = self.initial_lr + (self.max_lr - self.initial_lr) * \\\n",
    "                 (1 + math.cos(math.pi * round_num / 10)) / 2\n",
    "            \n",
    "        elif self.schedule_type == \"step\":\n",
    "            # Step increase every few rounds\n",
    "            if round_num <= 2:\n",
    "                lr = self.initial_lr\n",
    "            elif round_num <= 5:\n",
    "                lr = self.initial_lr * 2\n",
    "            elif round_num <= 8:\n",
    "                lr = self.initial_lr * 3\n",
    "            else:\n",
    "                lr = self.initial_lr * 4\n",
    "                \n",
    "        else:\n",
    "            lr = self.initial_lr\n",
    "        \n",
    "        # Cap at maximum learning rate\n",
    "        return min(lr, self.max_lr)\n",
    "\n",
    "# FedProx Loss Function\n",
    "class FedProxLoss(nn.Module):\n",
    "    def __init__(self, base_criterion, mu=0.01):\n",
    "        super(FedProxLoss, self).__init__()\n",
    "        self.base_criterion = base_criterion\n",
    "        self.mu = mu\n",
    "        \n",
    "    def forward(self, y_pred, y_true, model_params, global_params):\n",
    "        base_loss = self.base_criterion(y_pred, y_true)\n",
    "        \n",
    "        proximal_term = 0.0\n",
    "        if global_params is not None:\n",
    "            for local_param, global_param in zip(model_params, global_params):\n",
    "                proximal_term += torch.sum((local_param - global_param) ** 2)\n",
    "            loss = base_loss + (self.mu / 2) * proximal_term\n",
    "            return loss\n",
    "        \n",
    "        return base_loss\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(data_path):\n",
    "    \"\"\"Load and preprocess Framingham Heart Study data\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        logger.info(f\"Loaded {data_path} with shape {df.shape}\")\n",
    "        \n",
    "        missing_values = df.isnull().sum().sum()\n",
    "        if missing_values > 0:\n",
    "            logger.info(f\"Found {missing_values} missing values, dropping rows\")\n",
    "            df.dropna(inplace=True)\n",
    "            logger.info(f\"Shape after dropping missing values: {df.shape}\")\n",
    "        \n",
    "        if \"TenYearCHD\" not in df.columns:\n",
    "            raise ValueError(\"Target column 'TenYearCHD' not found!\")\n",
    "            \n",
    "        X = df.drop(columns=[\"TenYearCHD\"])\n",
    "        y = df[\"TenYearCHD\"]\n",
    "        \n",
    "        logger.info(f\"Class distribution: {y.value_counts().to_dict()}\")\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=CLIENT_CONFIG[\"batch_size\"], shuffle=True)\n",
    "        \n",
    "        logger.info(f\"Created dataloader with {len(dataset)} samples and {X.shape[1]} features\")\n",
    "        return dataloader, X.shape[1]\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Enhanced Client with Learning Rate Scheduling\n",
    "class FraminghamClientWithLRSchedule(fl.client.NumPyClient):\n",
    "    def __init__(self, model, dataloader, device, client_id=0):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.client_id = client_id\n",
    "        self.global_params = None\n",
    "        self.current_round = 0\n",
    "        \n",
    "        # Initialize learning rate scheduler\n",
    "        self.lr_scheduler = LearningRateScheduler(\n",
    "            initial_lr=CLIENT_CONFIG[\"initial_learning_rate\"],\n",
    "            schedule_type=CLIENT_CONFIG[\"lr_schedule\"],\n",
    "            factor=CLIENT_CONFIG[\"lr_factor\"],\n",
    "            max_lr=CLIENT_CONFIG[\"max_learning_rate\"]\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Initialized client {client_id} with device: {device}\")\n",
    "        logger.info(f\"LR Schedule: {CLIENT_CONFIG['lr_schedule']}\")\n",
    "        logger.info(f\"Initial LR: {CLIENT_CONFIG['initial_learning_rate']}\")\n",
    "        logger.info(f\"Max LR: {CLIENT_CONFIG['max_learning_rate']}\")\n",
    "        \n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Get model parameters\"\"\"\n",
    "        return [val.detach().cpu().numpy() for val in self.model.parameters()]\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"Set model parameters\"\"\"\n",
    "        self.global_params = [torch.tensor(p, device=self.device) for p in parameters]\n",
    "        \n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = {k: torch.tensor(v, device=self.device) for k, v in params_dict}\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "        \n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Train with dynamic learning rate\"\"\"\n",
    "        # Update model with server parameters\n",
    "        self.set_parameters(parameters)\n",
    "        \n",
    "        # Get current round number\n",
    "        self.current_round = config.get(\"server_round\", self.current_round + 1)\n",
    "        \n",
    "        # Calculate learning rate for this round\n",
    "        current_lr = self.lr_scheduler.get_lr(self.current_round)\n",
    "        \n",
    "        logger.info(f\"🔄 Round {self.current_round} - Learning Rate: {current_lr:.6f}\")\n",
    "        logger.info(f\"   LR Schedule: {CLIENT_CONFIG['lr_schedule']}\")\n",
    "        \n",
    "        # Train the model\n",
    "        self.model.train()\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        proximal_criterion = FedProxLoss(criterion, mu=CLIENT_CONFIG[\"proximal_mu\"])\n",
    "        \n",
    "        # Use dynamic learning rate\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), \n",
    "            lr=current_lr,  # Dynamic learning rate\n",
    "            weight_decay=CLIENT_CONFIG[\"weight_decay\"]\n",
    "        )\n",
    "        \n",
    "        # Training metrics\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        correct = 0\n",
    "        \n",
    "        # Train for multiple epochs\n",
    "        for epoch in range(CLIENT_CONFIG[\"local_epochs\"]):\n",
    "            epoch_loss = 0.0\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for batch_idx, (X, y) in enumerate(self.dataloader):\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                \n",
    "                y_pred = self.model(X)\n",
    "                loss = proximal_criterion(\n",
    "                    y_pred, y, \n",
    "                    self.model.parameters(),\n",
    "                    self.global_params\n",
    "                )\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_loss = loss.item() * X.size(0)\n",
    "                total_loss += batch_loss\n",
    "                epoch_loss += batch_loss\n",
    "                total_samples += X.size(0)\n",
    "                epoch_samples += X.size(0)\n",
    "                \n",
    "                predicted = (y_pred > 0.5).float()\n",
    "                correct += (predicted == y).sum().item()\n",
    "                \n",
    "                if batch_idx % 5 == 0:\n",
    "                    logger.info(\n",
    "                        f\"Epoch {epoch+1}/{CLIENT_CONFIG['local_epochs']} - \"\n",
    "                        f\"Batch {batch_idx}/{len(self.dataloader)} - \"\n",
    "                        f\"Loss: {loss.item():.4f} - LR: {current_lr:.6f}\"\n",
    "                    )\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Epoch {epoch+1}/{CLIENT_CONFIG['local_epochs']} completed - \"\n",
    "                f\"Loss: {epoch_loss/epoch_samples:.4f} - LR: {current_lr:.6f}\"\n",
    "            )\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "        accuracy = correct / total_samples if total_samples > 0 else 0\n",
    "        \n",
    "        logger.info(f\"Training completed - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, LR: {current_lr:.6f}\")\n",
    "        \n",
    "        # Return metrics including learning rate\n",
    "        metrics = {\n",
    "            \"loss\": float(avg_loss), \n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"learning_rate\": float(current_lr),\n",
    "            \"round\": self.current_round\n",
    "        }\n",
    "        \n",
    "        return self.get_parameters({}), total_samples, metrics\n",
    "    \n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluate the model\"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        \n",
    "        self.model.eval()\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                \n",
    "                y_pred = self.model(X)\n",
    "                batch_loss = criterion(y_pred, y).item()\n",
    "                \n",
    "                loss += batch_loss * X.size(0)\n",
    "                total += X.size(0)\n",
    "                \n",
    "                predicted = (y_pred > 0.5).float()\n",
    "                correct += (predicted == y).sum().item()\n",
    "        \n",
    "        avg_loss = loss / total if total > 0 else 0\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        \n",
    "        logger.info(f\"Evaluation - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return float(avg_loss), total, {\"accuracy\": float(accuracy)}\n",
    "\n",
    "def start_client(client_id=0, server_address=None):\n",
    "    \"\"\"Initialize and start client with learning rate scheduling\"\"\"\n",
    "    if server_address:\n",
    "        CLIENT_CONFIG[\"server_address\"] = server_address\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = f\"framingham_part{client_id+1}.csv\"\n",
    "    if not os.path.exists(data_path):\n",
    "        logger.error(f\"Data file {data_path} not found\")\n",
    "        return\n",
    "    \n",
    "    dataloader, input_size = load_data(data_path)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = HeartDiseaseModel(input_size=input_size).to(device)\n",
    "    logger.info(f\"Model initialized with input size: {input_size}\")\n",
    "    \n",
    "    # Create client with LR scheduling\n",
    "    client = FraminghamClientWithLRSchedule(model, dataloader, device, client_id)\n",
    "    \n",
    "    # Print configuration\n",
    "    print(f\"\\n===== Enhanced FL Client {client_id} (Dynamic Learning Rate) =====\")\n",
    "    print(f\"Server:           {CLIENT_CONFIG['server_address']}\")\n",
    "    print(f\"Data file:        {data_path}\")\n",
    "    print(f\"Local epochs:     {CLIENT_CONFIG['local_epochs']}\")\n",
    "    print(f\"LR Schedule:      {CLIENT_CONFIG['lr_schedule']}\")\n",
    "    print(f\"Initial LR:       {CLIENT_CONFIG['initial_learning_rate']}\")\n",
    "    print(f\"Max LR:           {CLIENT_CONFIG['max_learning_rate']}\")\n",
    "    print(f\"LR Factor:        {CLIENT_CONFIG['lr_factor']}\")\n",
    "    print(f\"Proximal mu:      {CLIENT_CONFIG['proximal_mu']}\")\n",
    "    print(f\"Device:           {device}\")\n",
    "    print(\"=\" * 65)\n",
    "    print(f\"\\nConnecting to server...\\n\")\n",
    "    \n",
    "    fl.client.start_client(server_address=CLIENT_CONFIG[\"server_address\"], client=client)\n",
    "\n",
    "# Auto-start when cell is executed\n",
    "print(\"🚀 Starting Enhanced Training Client with Learning Rate Scheduling...\")\n",
    "\n",
    "# You can modify these parameters\n",
    "CLIENT_CONFIG.update({\n",
    "    \"lr_schedule\": \"linear_increase\",  # Change to: \"exponential_increase\", \"cosine\", \"step\"\n",
    "    \"initial_learning_rate\": 0.0005,  # Start slower\n",
    "    \"max_learning_rate\": 0.005,       # Cap at reasonable level\n",
    "    \"lr_factor\": 1.3                  # Growth factor for exponential\n",
    "})\n",
    "\n",
    "start_client(client_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0b7d235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 22:05:14,584 - CompatibleBalancedClient - INFO - Using device: cpu\n",
      "2025-05-23 22:05:14,591 - CompatibleBalancedClient - INFO - Loaded framingham_part1.csv: (1060, 16)\n",
      "2025-05-23 22:05:14,594 - CompatibleBalancedClient - INFO - Class distribution: {0: 884, 1: 176}\n",
      "2025-05-23 22:05:14,596 - CompatibleBalancedClient - INFO - Healthy: 884 (83.4%)\n",
      "2025-05-23 22:05:14,598 - CompatibleBalancedClient - INFO - Disease: 176 (16.6%)\n",
      "2025-05-23 22:05:14,601 - CompatibleBalancedClient - INFO - Calculated class weights: {0: 0.5995475113122172, 1: 3.0113636363636362}\n",
      "2025-05-23 22:05:14,612 - CompatibleBalancedClient - INFO - Created BALANCED dataloader with weighted sampling\n",
      "2025-05-23 22:05:14,614 - CompatibleBalancedClient - INFO - Compatible model initialized with input size: 15\n",
      "2025-05-23 22:05:14,615 - CompatibleBalancedClient - INFO - 🎯 Compatible Balanced Client 0 initialized\n",
      "2025-05-23 22:05:14,617 - CompatibleBalancedClient - INFO -    Class weights: {0: 0.5995475113122172, 1: 3.0113636363636362}\n",
      "2025-05-23 22:05:14,618 - CompatibleBalancedClient - INFO -    Focal loss: True\n",
      "2025-05-23 22:05:14,619 - CompatibleBalancedClient - INFO -    Decision threshold: 0.3\n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-05-23 22:05:14,620 - flwr - WARNING - DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-05-23 22:05:14,624 - flwr - DEBUG - Opened insecure gRPC connection (no certificates were passed)\n",
      "2025-05-23 22:05:14,645 - flwr - DEBUG - ChannelConnectivity.IDLE\n",
      "2025-05-23 22:05:14,649 - flwr - DEBUG - ChannelConnectivity.READY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 COMPATIBLE BALANCED FEDERATED CLIENT\n",
      "============================================================\n",
      "✅ Works with your existing server architecture\n",
      "🎯 Enhanced with class balancing for better disease detection\n",
      "============================================================\n",
      "\n",
      "🎯 COMPATIBLE BALANCED FL Client 0\n",
      "============================================================\n",
      "✅ SAME architecture as your existing server\n",
      "🎯 ENHANCED with class balancing techniques\n",
      "============================================================\n",
      "Server: localhost:8080\n",
      "Data: framingham_part1.csv\n",
      "Device: cpu\n",
      "Class weights: {0: 0.5995475113122172, 1: 3.0113636363636362}\n",
      "Decision threshold: 0.3\n",
      "Focal loss: True\n",
      "Balanced sampling: True\n",
      "============================================================\n",
      "🚀 Expected improvements:\n",
      "   📈 Much higher recall (disease detection)\n",
      "   🏆 Better F1-score\n",
      "   ⚖️ Balanced precision/recall\n",
      "   🏥 Clinically useful performance\n",
      "\n",
      "🔌 Connecting to existing server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:05:18,325 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 5b886ae5-3b3f-46fd-b5ac-ce02c2de72de\n",
      "2025-05-23 22:05:18,328 - flwr - INFO - Received: train message 5b886ae5-3b3f-46fd-b5ac-ce02c2de72de\n",
      "\u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:05:18,330 - flwr - WARNING - Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:05:18,335 - CompatibleBalancedClient - INFO - 🔄 Round 1 - Compatible Balanced Training\n",
      "2025-05-23 22:05:18,336 - CompatibleBalancedClient - INFO -    Learning Rate: 0.001400\n",
      "2025-05-23 22:05:18,336 - CompatibleBalancedClient - INFO -    Threshold: 0.3\n",
      "2025-05-23 22:05:18,864 - CompatibleBalancedClient - INFO - 📊 BALANCED Training Results (Round 1):\n",
      "2025-05-23 22:05:18,865 - CompatibleBalancedClient - INFO -    Loss: 0.3342\n",
      "2025-05-23 22:05:18,866 - CompatibleBalancedClient - INFO -    Accuracy: 0.4947 (49.5%)\n",
      "2025-05-23 22:05:18,869 - CompatibleBalancedClient - INFO -    🎯 Recall: 1.0000 (100.0%) - DISEASE DETECTION\n",
      "2025-05-23 22:05:18,870 - CompatibleBalancedClient - INFO -    Precision: 0.4947 (49.5%)\n",
      "2025-05-23 22:05:18,871 - CompatibleBalancedClient - INFO -    🏆 F1-Score: 0.6619\n",
      "2025-05-23 22:05:18,872 - CompatibleBalancedClient - INFO -    Specificity: 0.0000 (0.0%)\n",
      "2025-05-23 22:05:18,874 - CompatibleBalancedClient - INFO -    Confusion Matrix: TP=1573, TN=0, FP=1607, FN=0\n",
      "2025-05-23 22:05:18,876 - CompatibleBalancedClient - INFO -    ✅ EXCELLENT disease detection improvement!\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-05-23 22:05:18,878 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:05:18,982 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 975f9a2a-f2cd-45c8-b0c6-552c61c36637\n",
      "2025-05-23 22:05:18,985 - flwr - INFO - Received: train message 975f9a2a-f2cd-45c8-b0c6-552c61c36637\n",
      "\u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:05:18,988 - flwr - WARNING - Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:05:18,995 - CompatibleBalancedClient - INFO - 🔄 Round 2 - Compatible Balanced Training\n",
      "2025-05-23 22:05:18,997 - CompatibleBalancedClient - INFO -    Learning Rate: 0.001800\n",
      "2025-05-23 22:05:18,998 - CompatibleBalancedClient - INFO -    Threshold: 0.3\n",
      "2025-05-23 22:05:19,381 - CompatibleBalancedClient - INFO - 📊 BALANCED Training Results (Round 2):\n",
      "2025-05-23 22:05:19,382 - CompatibleBalancedClient - INFO -    Loss: 0.3133\n",
      "2025-05-23 22:05:19,383 - CompatibleBalancedClient - INFO -    Accuracy: 0.5204 (52.0%)\n",
      "2025-05-23 22:05:19,384 - CompatibleBalancedClient - INFO -    🎯 Recall: 0.9994 (99.9%) - DISEASE DETECTION\n",
      "2025-05-23 22:05:19,385 - CompatibleBalancedClient - INFO -    Precision: 0.5153 (51.5%)\n",
      "2025-05-23 22:05:19,387 - CompatibleBalancedClient - INFO -    🏆 F1-Score: 0.6800\n",
      "2025-05-23 22:05:19,388 - CompatibleBalancedClient - INFO -    Specificity: 0.0225 (2.2%)\n",
      "2025-05-23 22:05:19,389 - CompatibleBalancedClient - INFO -    Confusion Matrix: TP=1620, TN=35, FP=1524, FN=1\n",
      "2025-05-23 22:05:19,391 - CompatibleBalancedClient - INFO -    ✅ EXCELLENT disease detection improvement!\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-05-23 22:05:19,394 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:05:19,472 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message e60001da-3769-41d1-b0a6-45a778753ffd\n",
      "2025-05-23 22:05:19,475 - flwr - INFO - Received: train message e60001da-3769-41d1-b0a6-45a778753ffd\n",
      "\u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:05:19,478 - flwr - WARNING - Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:05:19,483 - CompatibleBalancedClient - INFO - 🔄 Round 3 - Compatible Balanced Training\n",
      "2025-05-23 22:05:19,484 - CompatibleBalancedClient - INFO -    Learning Rate: 0.002200\n",
      "2025-05-23 22:05:19,486 - CompatibleBalancedClient - INFO -    Threshold: 0.3\n",
      "2025-05-23 22:05:19,871 - CompatibleBalancedClient - INFO - 📊 BALANCED Training Results (Round 3):\n",
      "2025-05-23 22:05:19,872 - CompatibleBalancedClient - INFO -    Loss: 0.3065\n",
      "2025-05-23 22:05:19,874 - CompatibleBalancedClient - INFO -    Accuracy: 0.5233 (52.3%)\n",
      "2025-05-23 22:05:19,875 - CompatibleBalancedClient - INFO -    🎯 Recall: 0.9949 (99.5%) - DISEASE DETECTION\n",
      "2025-05-23 22:05:19,875 - CompatibleBalancedClient - INFO -    Precision: 0.5099 (51.0%)\n",
      "2025-05-23 22:05:19,877 - CompatibleBalancedClient - INFO -    🏆 F1-Score: 0.6743\n",
      "2025-05-23 22:05:19,878 - CompatibleBalancedClient - INFO -    Specificity: 0.0593 (5.9%)\n",
      "2025-05-23 22:05:19,880 - CompatibleBalancedClient - INFO -    Confusion Matrix: TP=1569, TN=95, FP=1508, FN=8\n",
      "2025-05-23 22:05:19,882 - CompatibleBalancedClient - INFO -    ✅ EXCELLENT disease detection improvement!\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-05-23 22:05:19,885 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:05:20,075 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 4a8a062b-edc8-42a0-b218-fea96152e865\n",
      "2025-05-23 22:05:20,078 - flwr - INFO - Received: train message 4a8a062b-edc8-42a0-b218-fea96152e865\n",
      "\u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:05:20,081 - flwr - WARNING - Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:05:20,086 - CompatibleBalancedClient - INFO - 🔄 Round 4 - Compatible Balanced Training\n",
      "2025-05-23 22:05:20,087 - CompatibleBalancedClient - INFO -    Learning Rate: 0.002600\n",
      "2025-05-23 22:05:20,088 - CompatibleBalancedClient - INFO -    Threshold: 0.3\n",
      "2025-05-23 22:05:20,498 - CompatibleBalancedClient - INFO - 📊 BALANCED Training Results (Round 4):\n",
      "2025-05-23 22:05:20,500 - CompatibleBalancedClient - INFO -    Loss: 0.3062\n",
      "2025-05-23 22:05:20,501 - CompatibleBalancedClient - INFO -    Accuracy: 0.5465 (54.7%)\n",
      "2025-05-23 22:05:20,502 - CompatibleBalancedClient - INFO -    🎯 Recall: 0.9951 (99.5%) - DISEASE DETECTION\n",
      "2025-05-23 22:05:20,503 - CompatibleBalancedClient - INFO -    Precision: 0.5326 (53.3%)\n",
      "2025-05-23 22:05:20,504 - CompatibleBalancedClient - INFO -    🏆 F1-Score: 0.6938\n",
      "2025-05-23 22:05:20,506 - CompatibleBalancedClient - INFO -    Specificity: 0.0676 (6.8%)\n",
      "2025-05-23 22:05:20,508 - CompatibleBalancedClient - INFO -    Confusion Matrix: TP=1634, TN=104, FP=1434, FN=8\n",
      "2025-05-23 22:05:20,509 - CompatibleBalancedClient - INFO -    ✅ EXCELLENT disease detection improvement!\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-05-23 22:05:20,513 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:05:20,601 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 38fdaed9-fc0a-46a0-be82-75ca6bfb10d0\n",
      "2025-05-23 22:05:20,604 - flwr - INFO - Received: train message 38fdaed9-fc0a-46a0-be82-75ca6bfb10d0\n",
      "\u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:05:20,607 - flwr - WARNING - Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:05:20,614 - CompatibleBalancedClient - INFO - 🔄 Round 5 - Compatible Balanced Training\n",
      "2025-05-23 22:05:20,616 - CompatibleBalancedClient - INFO -    Learning Rate: 0.003000\n",
      "2025-05-23 22:05:20,618 - CompatibleBalancedClient - INFO -    Threshold: 0.3\n",
      "2025-05-23 22:05:21,140 - CompatibleBalancedClient - INFO - 📊 BALANCED Training Results (Round 5):\n",
      "2025-05-23 22:05:21,142 - CompatibleBalancedClient - INFO -    Loss: 0.3016\n",
      "2025-05-23 22:05:21,144 - CompatibleBalancedClient - INFO -    Accuracy: 0.5406 (54.1%)\n",
      "2025-05-23 22:05:21,146 - CompatibleBalancedClient - INFO -    🎯 Recall: 0.9969 (99.7%) - DISEASE DETECTION\n",
      "2025-05-23 22:05:21,148 - CompatibleBalancedClient - INFO -    Precision: 0.5211 (52.1%)\n",
      "2025-05-23 22:05:21,150 - CompatibleBalancedClient - INFO -    🏆 F1-Score: 0.6844\n",
      "2025-05-23 22:05:21,153 - CompatibleBalancedClient - INFO -    Specificity: 0.0849 (8.5%)\n",
      "2025-05-23 22:05:21,156 - CompatibleBalancedClient - INFO -    Confusion Matrix: TP=1584, TN=135, FP=1456, FN=5\n",
      "2025-05-23 22:05:21,157 - CompatibleBalancedClient - INFO -    ✅ EXCELLENT disease detection improvement!\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-05-23 22:05:21,162 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:05:21,309 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: reconnect message e26bd11a-4fb9-48de-b0c6-806515cf5104\n",
      "2025-05-23 22:05:21,313 - flwr - INFO - Received: reconnect message e26bd11a-4fb9-48de-b0c6-806515cf5104\n",
      "2025-05-23 22:05:21,357 - flwr - DEBUG - gRPC channel closed\n",
      "\u001b[92mINFO \u001b[0m:      Disconnect and shut down\n",
      "2025-05-23 22:05:21,359 - flwr - INFO - Disconnect and shut down\n"
     ]
    }
   ],
   "source": [
    "# Compatible Balanced Client - Same Architecture as Original Server\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"CompatibleBalancedClient\")\n",
    "\n",
    "# Configuration\n",
    "CLIENT_CONFIG = {\n",
    "    \"local_epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"initial_learning_rate\": 0.001,\n",
    "    \"lr_schedule\": \"linear_increase\",\n",
    "    \"lr_factor\": 1.2,\n",
    "    \"max_learning_rate\": 0.005,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"server_address\": \"localhost:8080\",\n",
    "    \"proximal_mu\": 0.01,\n",
    "    \n",
    "    # Class imbalance fixes\n",
    "    \"use_class_weights\": True,\n",
    "    \"balance_sampling\": True,\n",
    "    \"focal_loss\": True,\n",
    "    \"decision_threshold\": 0.3,  # Lower threshold for disease detection\n",
    "}\n",
    "\n",
    "# SAME MODEL ARCHITECTURE AS YOUR ORIGINAL SERVER\n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size=15):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Focal Loss for handling class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=2, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Weighted FedProx Loss\n",
    "class WeightedFedProxLoss(nn.Module):\n",
    "    def __init__(self, class_weights, use_focal=True, mu=0.01):\n",
    "        super(WeightedFedProxLoss, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.use_focal = use_focal\n",
    "        \n",
    "        if use_focal:\n",
    "            self.base_criterion = FocalLoss(alpha=2, gamma=2)\n",
    "        else:\n",
    "            # Weighted BCE\n",
    "            pos_weight = torch.tensor([class_weights[1] / class_weights[0]]) if class_weights else None\n",
    "            self.base_criterion = nn.BCELoss()\n",
    "        \n",
    "    def forward(self, y_pred, y_true, model_params=None, global_params=None):\n",
    "        # Base loss\n",
    "        base_loss = self.base_criterion(y_pred, y_true)\n",
    "        \n",
    "        # Proximal term\n",
    "        if global_params is not None and model_params is not None:\n",
    "            proximal_term = 0.0\n",
    "            for local_param, global_param in zip(model_params, global_params):\n",
    "                proximal_term += torch.sum((local_param - global_param) ** 2)\n",
    "            return base_loss + (self.mu / 2) * proximal_term\n",
    "        \n",
    "        return base_loss\n",
    "\n",
    "def load_balanced_data(data_path):\n",
    "    \"\"\"Load data with class balancing\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        logger.info(f\"Loaded {data_path}: {df.shape}\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        if df.isnull().sum().sum() > 0:\n",
    "            df.dropna(inplace=True)\n",
    "            logger.info(f\"After cleaning: {df.shape}\")\n",
    "        \n",
    "        if \"TenYearCHD\" not in df.columns:\n",
    "            raise ValueError(\"Target column 'TenYearCHD' not found!\")\n",
    "        \n",
    "        X = df.drop(columns=[\"TenYearCHD\"])\n",
    "        y = df[\"TenYearCHD\"]\n",
    "        \n",
    "        # Log original distribution\n",
    "        class_counts = y.value_counts().to_dict()\n",
    "        logger.info(f\"Class distribution: {class_counts}\")\n",
    "        logger.info(f\"Healthy: {class_counts.get(0, 0)} ({class_counts.get(0, 0)/len(y)*100:.1f}%)\")\n",
    "        logger.info(f\"Disease: {class_counts.get(1, 0)} ({class_counts.get(1, 0)/len(y)*100:.1f}%)\")\n",
    "        \n",
    "        # Calculate class weights\n",
    "        classes = np.unique(y)\n",
    "        class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "        class_weight_dict = dict(zip(classes, class_weights))\n",
    "        logger.info(f\"Calculated class weights: {class_weight_dict}\")\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        # Create balanced dataloader\n",
    "        if CLIENT_CONFIG[\"balance_sampling\"]:\n",
    "            # Weighted sampling for balanced batches\n",
    "            sample_weights = torch.tensor([class_weight_dict[int(label.item())] for label in y_tensor])\n",
    "            sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "            \n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            dataloader = DataLoader(dataset, batch_size=CLIENT_CONFIG[\"batch_size\"], sampler=sampler)\n",
    "            logger.info(\"Created BALANCED dataloader with weighted sampling\")\n",
    "        else:\n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            dataloader = DataLoader(dataset, batch_size=CLIENT_CONFIG[\"batch_size\"], shuffle=True)\n",
    "            logger.info(\"Created standard dataloader\")\n",
    "        \n",
    "        return dataloader, X.shape[1], class_weight_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "class LearningRateScheduler:\n",
    "    def __init__(self, initial_lr=0.001, max_lr=0.005):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.max_lr = max_lr\n",
    "    \n",
    "    def get_lr(self, round_num):\n",
    "        # Linear increase over rounds\n",
    "        step_size = (self.max_lr - self.initial_lr) / 10\n",
    "        lr = self.initial_lr + (round_num * step_size)\n",
    "        return min(lr, self.max_lr)\n",
    "\n",
    "# Compatible Balanced Client\n",
    "class CompatibleBalancedClient(fl.client.NumPyClient):\n",
    "    def __init__(self, model, dataloader, device, class_weights, client_id=0):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.client_id = client_id\n",
    "        self.class_weights = class_weights\n",
    "        self.global_params = None\n",
    "        self.current_round = 0\n",
    "        \n",
    "        self.lr_scheduler = LearningRateScheduler(\n",
    "            initial_lr=CLIENT_CONFIG[\"initial_learning_rate\"],\n",
    "            max_lr=CLIENT_CONFIG[\"max_learning_rate\"]\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"🎯 Compatible Balanced Client {client_id} initialized\")\n",
    "        logger.info(f\"   Class weights: {class_weights}\")\n",
    "        logger.info(f\"   Focal loss: {CLIENT_CONFIG['focal_loss']}\")\n",
    "        logger.info(f\"   Decision threshold: {CLIENT_CONFIG['decision_threshold']}\")\n",
    "    \n",
    "    def get_parameters(self, config):\n",
    "        return [val.detach().cpu().numpy() for val in self.model.parameters()]\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        # Store global parameters for FedProx\n",
    "        self.global_params = [torch.tensor(p, device=self.device) for p in parameters]\n",
    "        \n",
    "        # Update model parameters\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = {k: torch.tensor(v, device=self.device) for k, v in params_dict}\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Balanced training with compatible architecture\"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        self.current_round = config.get(\"server_round\", self.current_round + 1)\n",
    "        \n",
    "        # Get learning rate\n",
    "        current_lr = self.lr_scheduler.get_lr(self.current_round)\n",
    "        \n",
    "        logger.info(f\"🔄 Round {self.current_round} - Compatible Balanced Training\")\n",
    "        logger.info(f\"   Learning Rate: {current_lr:.6f}\")\n",
    "        logger.info(f\"   Threshold: {CLIENT_CONFIG['decision_threshold']}\")\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        # Create balanced loss\n",
    "        criterion = WeightedFedProxLoss(\n",
    "            class_weights=self.class_weights,\n",
    "            use_focal=CLIENT_CONFIG[\"focal_loss\"],\n",
    "            mu=CLIENT_CONFIG[\"proximal_mu\"]\n",
    "        )\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=current_lr,\n",
    "            weight_decay=CLIENT_CONFIG[\"weight_decay\"]\n",
    "        )\n",
    "        \n",
    "        # Training metrics\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        correct = 0\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_negatives = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(CLIENT_CONFIG[\"local_epochs\"]):\n",
    "            for batch_idx, (X, y) in enumerate(self.dataloader):\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = self.model(X)\n",
    "                \n",
    "                # Balanced loss with FedProx\n",
    "                loss = criterion(\n",
    "                    y_pred, y,\n",
    "                    list(self.model.parameters()),\n",
    "                    self.global_params\n",
    "                )\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Metrics with custom threshold\n",
    "                threshold = CLIENT_CONFIG[\"decision_threshold\"]\n",
    "                predictions = (y_pred > threshold).float()\n",
    "                \n",
    "                batch_size = X.size(0)\n",
    "                total_loss += loss.item() * batch_size\n",
    "                total_samples += batch_size\n",
    "                correct += (predictions == y).sum().item()\n",
    "                \n",
    "                # Detailed metrics\n",
    "                true_positives += ((predictions == 1) & (y == 1)).sum().item()\n",
    "                false_positives += ((predictions == 1) & (y == 0)).sum().item()\n",
    "                true_negatives += ((predictions == 0) & (y == 0)).sum().item()\n",
    "                false_negatives += ((predictions == 0) & (y == 1)).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "        accuracy = correct / total_samples if total_samples > 0 else 0\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        specificity = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n",
    "        \n",
    "        logger.info(f\"📊 BALANCED Training Results (Round {self.current_round}):\")\n",
    "        logger.info(f\"   Loss: {avg_loss:.4f}\")\n",
    "        logger.info(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "        logger.info(f\"   🎯 Recall: {recall:.4f} ({recall*100:.1f}%) - DISEASE DETECTION\")\n",
    "        logger.info(f\"   Precision: {precision:.4f} ({precision*100:.1f}%)\")\n",
    "        logger.info(f\"   🏆 F1-Score: {f1_score:.4f}\")\n",
    "        logger.info(f\"   Specificity: {specificity:.4f} ({specificity*100:.1f}%)\")\n",
    "        logger.info(f\"   Confusion Matrix: TP={true_positives}, TN={true_negatives}, FP={false_positives}, FN={false_negatives}\")\n",
    "        \n",
    "        # Compare to previous performance\n",
    "        if recall > 0.5:\n",
    "            logger.info(\"   ✅ EXCELLENT disease detection improvement!\")\n",
    "        elif recall > 0.3:\n",
    "            logger.info(\"   👍 GOOD disease detection improvement!\")\n",
    "        elif recall > 0.1:\n",
    "            logger.info(\"   📈 MODERATE improvement in disease detection\")\n",
    "        else:\n",
    "            logger.info(\"   ⚠️ Still working on improving disease detection...\")\n",
    "        \n",
    "        return self.get_parameters({}), total_samples, {\n",
    "            \"loss\": float(avg_loss),\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1_score\": float(f1_score),\n",
    "            \"specificity\": float(specificity),\n",
    "            \"learning_rate\": float(current_lr),\n",
    "            \"round\": self.current_round\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluation with balanced metrics\"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.eval()\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        tp = fp = tn = fn = 0\n",
    "        \n",
    "        threshold = CLIENT_CONFIG[\"decision_threshold\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in self.dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                \n",
    "                y_pred = self.model(X)\n",
    "                loss += criterion(y_pred, y).item() * X.size(0)\n",
    "                total += X.size(0)\n",
    "                \n",
    "                predictions = (y_pred > threshold).float()\n",
    "                correct += (predictions == y).sum().item()\n",
    "                \n",
    "                tp += ((predictions == 1) & (y == 1)).sum().item()\n",
    "                fp += ((predictions == 1) & (y == 0)).sum().item()\n",
    "                tn += ((predictions == 0) & (y == 0)).sum().item()\n",
    "                fn += ((predictions == 0) & (y == 1)).sum().item()\n",
    "        \n",
    "        avg_loss = loss / total if total > 0 else 0\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        return float(avg_loss), total, {\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall)\n",
    "        }\n",
    "\n",
    "def start_compatible_balanced_client(client_id=0, server_address=None):\n",
    "    \"\"\"Start compatible balanced client\"\"\"\n",
    "    if server_address:\n",
    "        CLIENT_CONFIG[\"server_address\"] = server_address\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = f\"framingham_part{client_id+1}.csv\"\n",
    "    if not os.path.exists(data_path):\n",
    "        logger.error(f\"Data file {data_path} not found\")\n",
    "        return\n",
    "    \n",
    "    dataloader, input_size, class_weights = load_balanced_data(data_path)\n",
    "    \n",
    "    # Use SAME model architecture as server\n",
    "    model = HeartDiseaseModel(input_size=input_size).to(device)\n",
    "    logger.info(f\"Compatible model initialized with input size: {input_size}\")\n",
    "    \n",
    "    # Create client\n",
    "    client = CompatibleBalancedClient(model, dataloader, device, class_weights, client_id)\n",
    "    \n",
    "    print(f\"\\n🎯 COMPATIBLE BALANCED FL Client {client_id}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"✅ SAME architecture as your existing server\")\n",
    "    print(f\"🎯 ENHANCED with class balancing techniques\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Server: {CLIENT_CONFIG['server_address']}\")\n",
    "    print(f\"Data: {data_path}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    print(f\"Decision threshold: {CLIENT_CONFIG['decision_threshold']}\")\n",
    "    print(f\"Focal loss: {CLIENT_CONFIG['focal_loss']}\")\n",
    "    print(f\"Balanced sampling: {CLIENT_CONFIG['balance_sampling']}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🚀 Expected improvements:\")\n",
    "    print(\"   📈 Much higher recall (disease detection)\")\n",
    "    print(\"   🏆 Better F1-score\")\n",
    "    print(\"   ⚖️ Balanced precision/recall\")\n",
    "    print(\"   🏥 Clinically useful performance\")\n",
    "    print(\"\\n🔌 Connecting to existing server...\")\n",
    "    \n",
    "    fl.client.start_client(server_address=CLIENT_CONFIG[\"server_address\"], client=client)\n",
    "\n",
    "# Start the compatible balanced client\n",
    "print(\"🎯 COMPATIBLE BALANCED FEDERATED CLIENT\")\n",
    "print(\"=\" * 60)\n",
    "print(\"✅ Works with your existing server architecture\")\n",
    "print(\"🎯 Enhanced with class balancing for better disease detection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_compatible_balanced_client(client_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81cc4f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 22:44:59,114 - CompatibleImprovedClient - INFO - Using device: cpu\n",
      "2025-05-23 22:44:59,114 - CompatibleImprovedClient - INFO - Loaded framingham_part1.csv: (1060, 16)\n",
      "2025-05-23 22:44:59,121 - CompatibleImprovedClient - INFO - Class distribution: {0: 884, 1: 176}\n",
      "2025-05-23 22:44:59,122 - CompatibleImprovedClient - INFO - Imbalance ratio (healthy:disease): 5.02:1\n",
      "2025-05-23 22:44:59,125 - CompatibleImprovedClient - INFO - Capped class weights: {0: 0.5995475113122172, 1: 3.0}\n",
      "2025-05-23 22:44:59,130 - CompatibleImprovedClient - INFO - Train set: 848 samples\n",
      "2025-05-23 22:44:59,132 - CompatibleImprovedClient - INFO - Validation set: 212 samples\n",
      "2025-05-23 22:44:59,136 - CompatibleImprovedClient - INFO - Created ENHANCED balanced dataloader with conservative weights\n",
      "2025-05-23 22:44:59,138 - CompatibleImprovedClient - INFO - Compatible enhanced model initialized: input_size=15\n",
      "2025-05-23 22:44:59,139 - CompatibleImprovedClient - INFO - 🚀 Compatible Enhanced Client 0 initialized\n",
      "2025-05-23 22:44:59,141 - CompatibleImprovedClient - INFO -    ✅ SAME architecture as original server\n",
      "2025-05-23 22:44:59,142 - CompatibleImprovedClient - INFO -    🎯 Enhanced training with regularization\n",
      "2025-05-23 22:44:59,143 - CompatibleImprovedClient - INFO -    📊 Adaptive threshold optimization\n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-05-23 22:44:59,145 - flwr - WARNING - DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "2025-05-23 22:44:59,148 - flwr - DEBUG - Opened insecure gRPC connection (no certificates were passed)\n",
      "2025-05-23 22:44:59,169 - flwr - DEBUG - ChannelConnectivity.IDLE\n",
      "2025-05-23 22:44:59,173 - flwr - DEBUG - ChannelConnectivity.READY\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 COMPATIBLE ENHANCED FL CLIENT 0\n",
      "======================================================================\n",
      "✅ SAME architecture as your original server\n",
      "🎯 ENHANCED training techniques and regularization\n",
      "📊 Adaptive threshold optimization\n",
      "======================================================================\n",
      "Server: localhost:8080\n",
      "Device: cpu\n",
      "Class weights: {0: 0.5995475113122172, 1: 3.0}\n",
      "\n",
      "Enhancements:\n",
      "   📈 Better regularization (dropout, weight decay, gradient clipping)\n",
      "   🎯 Adaptive threshold finding\n",
      "   ⚖️ More conservative class balancing\n",
      "   🏥 Validation-based optimization\n",
      "   🛡️ Label smoothing and enhanced focal loss\n",
      "\n",
      "🔌 Connecting to your existing server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:45:02,621 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message fec29b1f-c493-4fe6-991c-00dae8414314\n",
      "2025-05-23 22:45:02,623 - flwr - INFO - Received: train message fec29b1f-c493-4fe6-991c-00dae8414314\n",
      "\u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:45:02,624 - flwr - WARNING - Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:45:02,630 - CompatibleImprovedClient - INFO - 🔄 Round 1 - Compatible Enhanced Training\n",
      "2025-05-23 22:45:02,631 - CompatibleImprovedClient - INFO -    Learning Rate: 0.000500\n",
      "2025-05-23 22:45:02,632 - CompatibleImprovedClient - INFO -    Architecture: ✅ SAME as original server\n",
      "2025-05-23 22:45:02,716 - CompatibleImprovedClient - INFO -    Epoch 1: Loss = 0.2415\n",
      "2025-05-23 22:45:02,789 - CompatibleImprovedClient - INFO -    Epoch 2: Loss = 0.2440\n",
      "2025-05-23 22:45:02,832 - CompatibleImprovedClient - INFO - 📊 Compatible Enhanced Results (Round 1):\n",
      "2025-05-23 22:45:02,832 - CompatibleImprovedClient - INFO -    🎯 Optimal Threshold: 0.520\n",
      "2025-05-23 22:45:02,832 - CompatibleImprovedClient - INFO -    Training Loss: 0.2427\n",
      "2025-05-23 22:45:02,837 - CompatibleImprovedClient - INFO -    Validation Loss: 0.6556\n",
      "2025-05-23 22:45:02,838 - CompatibleImprovedClient - INFO -    Training Accuracy: 0.5767 (57.7%)\n",
      "2025-05-23 22:45:02,840 - CompatibleImprovedClient - INFO -    🎯 Training Recall: 0.0566 (5.7%)\n",
      "2025-05-23 22:45:02,843 - CompatibleImprovedClient - INFO -    📊 Training Precision: 0.7000 (70.0%)\n",
      "2025-05-23 22:45:02,845 - CompatibleImprovedClient - INFO -    🏆 Training F1-Score: 0.1047\n",
      "2025-05-23 22:45:02,846 - CompatibleImprovedClient - INFO -    🛡️ Training Specificity: 0.9811 (98.1%)\n",
      "2025-05-23 22:45:02,847 - CompatibleImprovedClient - INFO -    📈 Validation Score: 0.5229\n",
      "2025-05-23 22:45:02,849 - CompatibleImprovedClient - INFO -    ✅ NEW BEST validation performance!\n",
      "2025-05-23 22:45:02,850 - CompatibleImprovedClient - INFO -    ⚠️ Still optimizing...\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-05-23 22:45:02,854 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:45:02,983 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 6ce7d3cd-5810-4ebe-b589-3087fd85b55e\n",
      "2025-05-23 22:45:02,986 - flwr - INFO - Received: train message 6ce7d3cd-5810-4ebe-b589-3087fd85b55e\n",
      "\u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:45:02,990 - flwr - WARNING - Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:45:02,998 - CompatibleImprovedClient - INFO - 🔄 Round 2 - Compatible Enhanced Training\n",
      "2025-05-23 22:45:02,998 - CompatibleImprovedClient - INFO -    Learning Rate: 0.001780\n",
      "2025-05-23 22:45:02,998 - CompatibleImprovedClient - INFO -    Architecture: ✅ SAME as original server\n",
      "2025-05-23 22:45:03,193 - CompatibleImprovedClient - INFO -    Epoch 1: Loss = 0.2362\n",
      "2025-05-23 22:45:03,252 - CompatibleImprovedClient - INFO -    Epoch 2: Loss = 0.2354\n",
      "2025-05-23 22:45:03,284 - CompatibleImprovedClient - INFO - 📊 Compatible Enhanced Results (Round 2):\n",
      "2025-05-23 22:45:03,285 - CompatibleImprovedClient - INFO -    🎯 Optimal Threshold: 0.550\n",
      "2025-05-23 22:45:03,287 - CompatibleImprovedClient - INFO -    Training Loss: 0.2358\n",
      "2025-05-23 22:45:03,287 - CompatibleImprovedClient - INFO -    Validation Loss: 0.6419\n",
      "2025-05-23 22:45:03,287 - CompatibleImprovedClient - INFO -    Training Accuracy: 0.6144 (61.4%)\n",
      "2025-05-23 22:45:03,287 - CompatibleImprovedClient - INFO -    🎯 Training Recall: 0.2010 (20.1%)\n",
      "2025-05-23 22:45:03,291 - CompatibleImprovedClient - INFO -    📊 Training Precision: 0.8211 (82.1%)\n",
      "2025-05-23 22:45:03,291 - CompatibleImprovedClient - INFO -    🏆 Training F1-Score: 0.3230\n",
      "2025-05-23 22:45:03,293 - CompatibleImprovedClient - INFO -    🛡️ Training Specificity: 0.9630 (96.3%)\n",
      "2025-05-23 22:45:03,294 - CompatibleImprovedClient - INFO -    📈 Validation Score: 0.5229\n",
      "2025-05-23 22:45:03,294 - CompatibleImprovedClient - INFO -    ⏳ Patience: 1/3\n",
      "2025-05-23 22:45:03,294 - CompatibleImprovedClient - INFO -    ⚠️ Still optimizing...\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-05-23 22:45:03,303 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:45:03,358 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message be063876-a31c-4ad7-ba96-31ce29f613c5\n",
      "2025-05-23 22:45:03,360 - flwr - INFO - Received: train message be063876-a31c-4ad7-ba96-31ce29f613c5\n",
      "\u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:45:03,363 - flwr - WARNING - Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:45:03,369 - CompatibleImprovedClient - INFO - 🔄 Round 3 - Compatible Enhanced Training\n",
      "2025-05-23 22:45:03,370 - CompatibleImprovedClient - INFO -    Learning Rate: 0.001250\n",
      "2025-05-23 22:45:03,372 - CompatibleImprovedClient - INFO -    Architecture: ✅ SAME as original server\n",
      "2025-05-23 22:45:03,449 - CompatibleImprovedClient - INFO -    Epoch 1: Loss = 0.2239\n",
      "2025-05-23 22:45:03,516 - CompatibleImprovedClient - INFO -    Epoch 2: Loss = 0.2298\n",
      "2025-05-23 22:45:03,557 - CompatibleImprovedClient - INFO - 📊 Compatible Enhanced Results (Round 3):\n",
      "2025-05-23 22:45:03,558 - CompatibleImprovedClient - INFO -    🎯 Optimal Threshold: 0.460\n",
      "2025-05-23 22:45:03,559 - CompatibleImprovedClient - INFO -    Training Loss: 0.2268\n",
      "2025-05-23 22:45:03,560 - CompatibleImprovedClient - INFO -    Validation Loss: 0.6269\n",
      "2025-05-23 22:45:03,562 - CompatibleImprovedClient - INFO -    Training Accuracy: 0.6745 (67.5%)\n",
      "2025-05-23 22:45:03,562 - CompatibleImprovedClient - INFO -    🎯 Training Recall: 0.7900 (79.0%)\n",
      "2025-05-23 22:45:03,564 - CompatibleImprovedClient - INFO -    📊 Training Precision: 0.6056 (60.6%)\n",
      "2025-05-23 22:45:03,565 - CompatibleImprovedClient - INFO -    🏆 Training F1-Score: 0.6856\n",
      "2025-05-23 22:45:03,567 - CompatibleImprovedClient - INFO -    🛡️ Training Specificity: 0.5803 (58.0%)\n",
      "2025-05-23 22:45:03,568 - CompatibleImprovedClient - INFO -    📈 Validation Score: 0.5190\n",
      "2025-05-23 22:45:03,568 - CompatibleImprovedClient - INFO -    ⏳ Patience: 2/3\n",
      "2025-05-23 22:45:03,571 - CompatibleImprovedClient - INFO -    ✅ GOOD balanced performance!\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-05-23 22:45:03,574 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:45:03,631 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message f7ce11c2-0b6d-4e57-b5f5-668442bdbff5\n",
      "2025-05-23 22:45:03,633 - flwr - INFO - Received: train message f7ce11c2-0b6d-4e57-b5f5-668442bdbff5\n",
      "\u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:45:03,636 - flwr - WARNING - Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:45:03,644 - CompatibleImprovedClient - INFO - 🔄 Round 4 - Compatible Enhanced Training\n",
      "2025-05-23 22:45:03,644 - CompatibleImprovedClient - INFO -    Learning Rate: 0.000720\n",
      "2025-05-23 22:45:03,644 - CompatibleImprovedClient - INFO -    Architecture: ✅ SAME as original server\n",
      "2025-05-23 22:45:03,714 - CompatibleImprovedClient - INFO -    Epoch 1: Loss = 0.2232\n",
      "2025-05-23 22:45:03,778 - CompatibleImprovedClient - INFO -    Epoch 2: Loss = 0.2253\n",
      "2025-05-23 22:45:03,820 - CompatibleImprovedClient - INFO - 📊 Compatible Enhanced Results (Round 4):\n",
      "2025-05-23 22:45:03,822 - CompatibleImprovedClient - INFO -    🎯 Optimal Threshold: 0.600\n",
      "2025-05-23 22:45:03,823 - CompatibleImprovedClient - INFO -    Training Loss: 0.2242\n",
      "2025-05-23 22:45:03,825 - CompatibleImprovedClient - INFO -    Validation Loss: 0.6306\n",
      "2025-05-23 22:45:03,826 - CompatibleImprovedClient - INFO -    Training Accuracy: 0.5849 (58.5%)\n",
      "2025-05-23 22:45:03,828 - CompatibleImprovedClient - INFO -    🎯 Training Recall: 0.1132 (11.3%)\n",
      "2025-05-23 22:45:03,829 - CompatibleImprovedClient - INFO -    📊 Training Precision: 0.7414 (74.1%)\n",
      "2025-05-23 22:45:03,831 - CompatibleImprovedClient - INFO -    🏆 Training F1-Score: 0.1963\n",
      "2025-05-23 22:45:03,832 - CompatibleImprovedClient - INFO -    🛡️ Training Specificity: 0.9679 (96.8%)\n",
      "2025-05-23 22:45:03,835 - CompatibleImprovedClient - INFO -    📈 Validation Score: 0.5229\n",
      "2025-05-23 22:45:03,837 - CompatibleImprovedClient - INFO -    ⏳ Patience: 3/3\n",
      "2025-05-23 22:45:03,838 - CompatibleImprovedClient - INFO -    ⚠️ Still optimizing...\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-05-23 22:45:03,843 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:45:03,990 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 88dc59bf-ac08-412f-932d-743f6788f8ec\n",
      "2025-05-23 22:45:03,992 - flwr - INFO - Received: train message 88dc59bf-ac08-412f-932d-743f6788f8ec\n",
      "\u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:45:03,995 - flwr - WARNING - Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
      "2025-05-23 22:45:04,002 - CompatibleImprovedClient - INFO - 🔄 Round 5 - Compatible Enhanced Training\n",
      "2025-05-23 22:45:04,003 - CompatibleImprovedClient - INFO -    Learning Rate: 0.000500\n",
      "2025-05-23 22:45:04,005 - CompatibleImprovedClient - INFO -    Architecture: ✅ SAME as original server\n",
      "2025-05-23 22:45:04,106 - CompatibleImprovedClient - INFO -    Epoch 1: Loss = 0.2253\n",
      "2025-05-23 22:45:04,219 - CompatibleImprovedClient - INFO -    Epoch 2: Loss = 0.2248\n",
      "2025-05-23 22:45:04,246 - CompatibleImprovedClient - INFO - 📊 Compatible Enhanced Results (Round 5):\n",
      "2025-05-23 22:45:04,247 - CompatibleImprovedClient - INFO -    🎯 Optimal Threshold: 0.450\n",
      "2025-05-23 22:45:04,249 - CompatibleImprovedClient - INFO -    Training Loss: 0.2250\n",
      "2025-05-23 22:45:04,250 - CompatibleImprovedClient - INFO -    Validation Loss: 0.6152\n",
      "2025-05-23 22:45:04,251 - CompatibleImprovedClient - INFO -    Training Accuracy: 0.6710 (67.1%)\n",
      "2025-05-23 22:45:04,251 - CompatibleImprovedClient - INFO -    🎯 Training Recall: 0.7895 (78.9%)\n",
      "2025-05-23 22:45:04,252 - CompatibleImprovedClient - INFO -    📊 Training Precision: 0.6012 (60.1%)\n",
      "2025-05-23 22:45:04,254 - CompatibleImprovedClient - INFO -    🏆 Training F1-Score: 0.6826\n",
      "2025-05-23 22:45:04,254 - CompatibleImprovedClient - INFO -    🛡️ Training Specificity: 0.5748 (57.5%)\n",
      "2025-05-23 22:45:04,254 - CompatibleImprovedClient - INFO -    📈 Validation Score: 0.5320\n",
      "2025-05-23 22:45:04,259 - CompatibleImprovedClient - INFO -    ✅ NEW BEST validation performance!\n",
      "2025-05-23 22:45:04,260 - CompatibleImprovedClient - INFO -    ✅ GOOD balanced performance!\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "2025-05-23 22:45:04,263 - flwr - INFO - Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "2025-05-23 22:45:04,322 - flwr - INFO - \n",
      "\u001b[92mINFO \u001b[0m:      Received: reconnect message 0efee359-1484-4878-9524-cf36353b0474\n",
      "2025-05-23 22:45:04,324 - flwr - INFO - Received: reconnect message 0efee359-1484-4878-9524-cf36353b0474\n",
      "2025-05-23 22:45:04,363 - flwr - DEBUG - gRPC channel closed\n",
      "\u001b[92mINFO \u001b[0m:      Disconnect and shut down\n",
      "2025-05-23 22:45:04,365 - flwr - INFO - Disconnect and shut down\n"
     ]
    }
   ],
   "source": [
    "# Compatible Improved FL Client - SAME Architecture with Enhanced Training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"CompatibleImprovedClient\")\n",
    "\n",
    "# Enhanced Configuration with Same Architecture\n",
    "CLIENT_CONFIG = {\n",
    "    \"local_epochs\": 2,  # Reduced to prevent overfitting\n",
    "    \"batch_size\": 64,   # Increased for better gradients\n",
    "    \"initial_learning_rate\": 0.0005,  # Lower initial LR\n",
    "    \"lr_schedule\": \"cosine_annealing\",\n",
    "    \"max_learning_rate\": 0.002,  # Lower max LR\n",
    "    \"weight_decay\": 1e-4,  # Increased regularization\n",
    "    \"server_address\": \"localhost:8080\",\n",
    "    \"proximal_mu\": 0.05,  # Balanced proximal term\n",
    "    \n",
    "    # Enhanced class balance handling\n",
    "    \"use_class_weights\": True,\n",
    "    \"balance_sampling\": True,\n",
    "    \"focal_loss\": True,\n",
    "    \"focal_alpha\": 1.0,  # More conservative\n",
    "    \"focal_gamma\": 1.5,  # More conservative\n",
    "    \"adaptive_threshold\": True,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \n",
    "    # Enhanced regularization (compatible)\n",
    "    \"dropout_rate\": 0.4,  # Increased dropout\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"label_smoothing\": 0.1,\n",
    "}\n",
    "\n",
    "# EXACT SAME MODEL ARCHITECTURE AS ORIGINAL SERVER - Enhanced Training Only\n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size=15):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # Keep original 0.3\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  # Keep original 0.3\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Enhanced Focal Loss with Label Smoothing\n",
    "class EnhancedFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=1.5, label_smoothing=0.1, reduction='mean'):\n",
    "        super(EnhancedFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Apply label smoothing to reduce overconfidence\n",
    "        if self.label_smoothing > 0:\n",
    "            targets = targets * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "        \n",
    "        bce_loss = nn.functional.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Adaptive Threshold Optimizer\n",
    "class ThresholdOptimizer:\n",
    "    def __init__(self):\n",
    "        self.best_threshold = 0.5\n",
    "        self.best_f1 = 0.0\n",
    "    \n",
    "    def find_optimal_threshold(self, y_true, y_probs, metric='balanced'):\n",
    "        \"\"\"Find optimal threshold with multiple strategies\"\"\"\n",
    "        y_true_np = y_true.detach().cpu().numpy().flatten()\n",
    "        y_probs_np = y_probs.detach().cpu().numpy().flatten()\n",
    "        \n",
    "        # More comprehensive threshold search\n",
    "        thresholds = np.linspace(0.1, 0.9, 81)\n",
    "        best_score = 0.0\n",
    "        best_thresh = 0.5\n",
    "        best_metrics = {}\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            y_pred = (y_probs_np > thresh).astype(int)\n",
    "            \n",
    "            # Calculate detailed metrics\n",
    "            tp = np.sum((y_pred == 1) & (y_true_np == 1))\n",
    "            fp = np.sum((y_pred == 1) & (y_true_np == 0))\n",
    "            tn = np.sum((y_pred == 0) & (y_true_np == 0))\n",
    "            fn = np.sum((y_pred == 0) & (y_true_np == 1))\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            \n",
    "            # Multiple scoring strategies\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            balanced_score = (precision + recall + specificity) / 3\n",
    "            gmean = np.sqrt(recall * specificity) if (recall * specificity) > 0 else 0\n",
    "            \n",
    "            # Choose best based on metric with minimum constraints\n",
    "            if precision >= 0.05 and recall >= 0.05:  # Minimum viable performance\n",
    "                if metric == 'f1' and f1 > best_score:\n",
    "                    best_score = f1\n",
    "                    best_thresh = thresh\n",
    "                    best_metrics = {\n",
    "                        'threshold': thresh, 'f1': f1, 'precision': precision,\n",
    "                        'recall': recall, 'specificity': specificity, 'balanced': balanced_score, 'gmean': gmean\n",
    "                    }\n",
    "                elif metric == 'balanced' and balanced_score > best_score:\n",
    "                    best_score = balanced_score\n",
    "                    best_thresh = thresh\n",
    "                    best_metrics = {\n",
    "                        'threshold': thresh, 'f1': f1, 'precision': precision,\n",
    "                        'recall': recall, 'specificity': specificity, 'balanced': balanced_score, 'gmean': gmean\n",
    "                    }\n",
    "                elif metric == 'gmean' and gmean > best_score:\n",
    "                    best_score = gmean\n",
    "                    best_thresh = thresh\n",
    "                    best_metrics = {\n",
    "                        'threshold': thresh, 'f1': f1, 'precision': precision,\n",
    "                        'recall': recall, 'specificity': specificity, 'balanced': balanced_score, 'gmean': gmean\n",
    "                    }\n",
    "        \n",
    "        if best_metrics:\n",
    "            self.best_threshold = best_thresh\n",
    "            self.best_f1 = best_metrics['f1']\n",
    "            return best_thresh, best_score, best_metrics\n",
    "        else:\n",
    "            # Fallback if no good threshold found\n",
    "            return 0.5, 0.0, {'threshold': 0.5, 'f1': 0.0, 'precision': 0.0, 'recall': 0.0, 'specificity': 0.0}\n",
    "\n",
    "# Enhanced Learning Rate Scheduler\n",
    "class EnhancedLRScheduler:\n",
    "    def __init__(self, initial_lr=0.0005, max_lr=0.002, total_rounds=5):\n",
    "        self.initial_lr = initial_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.total_rounds = total_rounds\n",
    "    \n",
    "    def get_lr(self, round_num):\n",
    "        # Cosine annealing with warm-up\n",
    "        if round_num <= 1:\n",
    "            return self.initial_lr\n",
    "        \n",
    "        progress = (round_num - 1) / max(self.total_rounds - 1, 1)\n",
    "        lr = self.initial_lr + (self.max_lr - self.initial_lr) * (1 + np.cos(np.pi * progress)) / 2\n",
    "        return lr\n",
    "\n",
    "# Enhanced Data Loading with Better Preprocessing\n",
    "def load_enhanced_balanced_data(data_path):\n",
    "    \"\"\"Load data with enhanced preprocessing and balancing\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        logger.info(f\"Loaded {data_path}: {df.shape}\")\n",
    "        \n",
    "        # Enhanced missing value handling\n",
    "        if df.isnull().sum().sum() > 0:\n",
    "            logger.info(\"Handling missing values...\")\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype in ['int64', 'float64']:\n",
    "                    # Use median for numerical\n",
    "                    df[col].fillna(df[col].median(), inplace=True)\n",
    "                else:\n",
    "                    # Use mode for categorical\n",
    "                    mode_val = df[col].mode()\n",
    "                    df[col].fillna(mode_val[0] if len(mode_val) > 0 else 0, inplace=True)\n",
    "            logger.info(f\"After imputation: {df.shape}\")\n",
    "        \n",
    "        if \"TenYearCHD\" not in df.columns:\n",
    "            raise ValueError(\"Target column 'TenYearCHD' not found!\")\n",
    "        \n",
    "        X = df.drop(columns=[\"TenYearCHD\"])\n",
    "        y = df[\"TenYearCHD\"]\n",
    "        \n",
    "        # Log class distribution\n",
    "        class_counts = y.value_counts().to_dict()\n",
    "        logger.info(f\"Class distribution: {class_counts}\")\n",
    "        imbalance_ratio = class_counts.get(0, 0) / max(class_counts.get(1, 1), 1)\n",
    "        logger.info(f\"Imbalance ratio (healthy:disease): {imbalance_ratio:.2f}:1\")\n",
    "        \n",
    "        # Enhanced class weight calculation with caps\n",
    "        classes = np.unique(y)\n",
    "        class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "        \n",
    "        # Cap extreme weights to prevent overfitting\n",
    "        max_weight = 3.0  # More conservative cap\n",
    "        class_weights = np.clip(class_weights, 0.2, max_weight)\n",
    "        class_weight_dict = dict(zip(classes, class_weights))\n",
    "        logger.info(f\"Capped class weights: {class_weight_dict}\")\n",
    "        \n",
    "        # Enhanced feature scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Train/validation split for threshold optimization\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_scaled, y.values, test_size=0.2, stratify=y, random_state=42\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Train set: {X_train.shape[0]} samples\")\n",
    "        logger.info(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        # Enhanced balanced sampling\n",
    "        if CLIENT_CONFIG[\"balance_sampling\"]:\n",
    "            # More conservative sample weights\n",
    "            sample_weights = torch.tensor([class_weight_dict[int(label.item())] for label in y_train_tensor])\n",
    "            sample_weights = torch.clamp(sample_weights, 0.5, 2.5)  # Conservative clamp\n",
    "            sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "            \n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=CLIENT_CONFIG[\"batch_size\"], sampler=sampler)\n",
    "            logger.info(\"Created ENHANCED balanced dataloader with conservative weights\")\n",
    "        else:\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=CLIENT_CONFIG[\"batch_size\"], shuffle=True)\n",
    "        \n",
    "        # Validation dataloader\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=CLIENT_CONFIG[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        return train_dataloader, val_dataloader, X.shape[1], class_weight_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Compatible Enhanced Client\n",
    "class CompatibleEnhancedClient(fl.client.NumPyClient):\n",
    "    def __init__(self, model, train_dataloader, val_dataloader, device, class_weights, client_id=0):\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.device = device\n",
    "        self.client_id = client_id\n",
    "        self.class_weights = class_weights\n",
    "        self.global_params = None\n",
    "        self.current_round = 0\n",
    "        \n",
    "        self.lr_scheduler = EnhancedLRScheduler(\n",
    "            initial_lr=CLIENT_CONFIG[\"initial_learning_rate\"],\n",
    "            max_lr=CLIENT_CONFIG[\"max_learning_rate\"]\n",
    "        )\n",
    "        \n",
    "        self.threshold_optimizer = ThresholdOptimizer()\n",
    "        self.best_val_score = 0.0\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "        logger.info(f\"🚀 Compatible Enhanced Client {client_id} initialized\")\n",
    "        logger.info(f\"   ✅ SAME architecture as original server\")\n",
    "        logger.info(f\"   🎯 Enhanced training with regularization\")\n",
    "        logger.info(f\"   📊 Adaptive threshold optimization\")\n",
    "    \n",
    "    def get_parameters(self, config):\n",
    "        return [val.detach().cpu().numpy() for val in self.model.parameters()]\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        # Store global parameters for FedProx\n",
    "        self.global_params = [torch.tensor(p, device=self.device) for p in parameters]\n",
    "        \n",
    "        # Update model parameters (same architecture, so this works)\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = {k: torch.tensor(v, device=self.device) for k, v in params_dict}\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "    \n",
    "    def validate_and_optimize_threshold(self):\n",
    "        \"\"\"Validate and find optimal threshold\"\"\"\n",
    "        self.model.eval()\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        val_loss = 0.0\n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in self.val_dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                y_pred = self.model(X)\n",
    "                loss = criterion(y_pred, y)\n",
    "                \n",
    "                all_probs.append(y_pred)\n",
    "                all_labels.append(y)\n",
    "                val_loss += loss.item() * X.size(0)\n",
    "        \n",
    "        if all_probs:\n",
    "            all_probs = torch.cat(all_probs)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            avg_val_loss = val_loss / len(all_labels)\n",
    "            \n",
    "            # Optimize threshold\n",
    "            optimal_thresh, best_score, metrics = self.threshold_optimizer.find_optimal_threshold(\n",
    "                all_labels, all_probs, metric='balanced'\n",
    "            )\n",
    "            \n",
    "            return optimal_thresh, best_score, metrics, avg_val_loss, all_probs, all_labels\n",
    "        \n",
    "        return 0.5, 0.0, {}, 0.0, None, None\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Enhanced training with same architecture\"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        self.current_round = config.get(\"server_round\", self.current_round + 1)\n",
    "        \n",
    "        current_lr = self.lr_scheduler.get_lr(self.current_round)\n",
    "        \n",
    "        logger.info(f\"🔄 Round {self.current_round} - Compatible Enhanced Training\")\n",
    "        logger.info(f\"   Learning Rate: {current_lr:.6f}\")\n",
    "        logger.info(f\"   Architecture: ✅ SAME as original server\")\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        # Enhanced loss function\n",
    "        criterion = EnhancedFocalLoss(\n",
    "            alpha=CLIENT_CONFIG[\"focal_alpha\"],\n",
    "            gamma=CLIENT_CONFIG[\"focal_gamma\"],\n",
    "            label_smoothing=CLIENT_CONFIG[\"label_smoothing\"]\n",
    "        )\n",
    "        \n",
    "        # Enhanced optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=current_lr,\n",
    "            weight_decay=CLIENT_CONFIG[\"weight_decay\"]\n",
    "        )\n",
    "        \n",
    "        # Training loop with enhanced techniques\n",
    "        total_loss = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for epoch in range(CLIENT_CONFIG[\"local_epochs\"]):\n",
    "            epoch_loss = 0.0\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for batch_idx, (X, y) in enumerate(self.train_dataloader):\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                y_pred = self.model(X)\n",
    "                \n",
    "                # Enhanced loss with FedProx\n",
    "                loss = criterion(y_pred, y)\n",
    "                \n",
    "                # FedProx proximal term\n",
    "                if self.global_params is not None:\n",
    "                    proximal_term = 0.0\n",
    "                    for local_param, global_param in zip(self.model.parameters(), self.global_params):\n",
    "                        proximal_term += torch.sum((local_param - global_param) ** 2)\n",
    "                    loss += (CLIENT_CONFIG[\"proximal_mu\"] / 2) * proximal_term\n",
    "                \n",
    "                # Backward pass with gradient clipping\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                if CLIENT_CONFIG[\"gradient_clipping\"]:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), CLIENT_CONFIG[\"gradient_clipping\"])\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                batch_size = X.size(0)\n",
    "                epoch_loss += loss.item() * batch_size\n",
    "                epoch_samples += batch_size\n",
    "            \n",
    "            total_loss += epoch_loss\n",
    "            total_samples += epoch_samples\n",
    "            \n",
    "            logger.info(f\"   Epoch {epoch+1}: Loss = {epoch_loss/epoch_samples:.4f}\")\n",
    "        \n",
    "        # Validation and threshold optimization\n",
    "        optimal_thresh, val_score, val_metrics, val_loss, _, _ = self.validate_and_optimize_threshold()\n",
    "        \n",
    "        # Calculate training metrics with optimal threshold\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "            train_tp = train_fp = train_tn = train_fn = 0\n",
    "            \n",
    "            for X, y in self.train_dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                y_pred = self.model(X)\n",
    "                \n",
    "                predictions = (y_pred > optimal_thresh).float()\n",
    "                train_correct += (predictions == y).sum().item()\n",
    "                train_total += y.size(0)\n",
    "                \n",
    "                train_tp += ((predictions == 1) & (y == 1)).sum().item()\n",
    "                train_fp += ((predictions == 1) & (y == 0)).sum().item()\n",
    "                train_tn += ((predictions == 0) & (y == 0)).sum().item()\n",
    "                train_fn += ((predictions == 0) & (y == 1)).sum().item()\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "        accuracy = train_correct / train_total if train_total > 0 else 0\n",
    "        \n",
    "        precision = train_tp / (train_tp + train_fp) if (train_tp + train_fp) > 0 else 0\n",
    "        recall = train_tp / (train_tp + train_fn) if (train_tp + train_fn) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        specificity = train_tn / (train_tn + train_fp) if (train_tn + train_fp) > 0 else 0\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_score > self.best_val_score:\n",
    "            self.best_val_score = val_score\n",
    "            self.patience_counter = 0\n",
    "            improvement_status = \"✅ NEW BEST validation performance!\"\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            improvement_status = f\"⏳ Patience: {self.patience_counter}/{CLIENT_CONFIG['early_stopping_patience']}\"\n",
    "        \n",
    "        logger.info(f\"📊 Compatible Enhanced Results (Round {self.current_round}):\")\n",
    "        logger.info(f\"   🎯 Optimal Threshold: {optimal_thresh:.3f}\")\n",
    "        logger.info(f\"   Training Loss: {avg_loss:.4f}\")\n",
    "        logger.info(f\"   Validation Loss: {val_loss:.4f}\")\n",
    "        logger.info(f\"   Training Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)\")\n",
    "        logger.info(f\"   🎯 Training Recall: {recall:.4f} ({recall*100:.1f}%)\")\n",
    "        logger.info(f\"   📊 Training Precision: {precision:.4f} ({precision*100:.1f}%)\")\n",
    "        logger.info(f\"   🏆 Training F1-Score: {f1_score:.4f}\")\n",
    "        logger.info(f\"   🛡️ Training Specificity: {specificity:.4f} ({specificity*100:.1f}%)\")\n",
    "        logger.info(f\"   📈 Validation Score: {val_score:.4f}\")\n",
    "        logger.info(f\"   {improvement_status}\")\n",
    "        \n",
    "        # Performance assessment\n",
    "        if recall >= 0.8 and precision >= 0.2:\n",
    "            logger.info(\"   🏆 EXCELLENT balanced performance!\")\n",
    "        elif recall >= 0.6 and precision >= 0.15:\n",
    "            logger.info(\"   ✅ GOOD balanced performance!\")\n",
    "        elif recall >= 0.4:\n",
    "            logger.info(\"   📈 MODERATE improvement\")\n",
    "        else:\n",
    "            logger.info(\"   ⚠️ Still optimizing...\")\n",
    "        \n",
    "        return self.get_parameters({}), total_samples, {\n",
    "            \"loss\": float(avg_loss),\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1_score\": float(f1_score),\n",
    "            \"specificity\": float(specificity),\n",
    "            \"val_score\": float(val_score),\n",
    "            \"optimal_threshold\": float(optimal_thresh),\n",
    "            \"learning_rate\": float(current_lr),\n",
    "            \"round\": self.current_round\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Enhanced evaluation with optimal threshold\"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        optimal_thresh, _, _, val_loss, _, _ = self.validate_and_optimize_threshold()\n",
    "        \n",
    "        self.model.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        tp = fp = tn = fn = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, y in self.val_dataloader:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "                y_pred = self.model(X)\n",
    "                \n",
    "                predictions = (y_pred > optimal_thresh).float()\n",
    "                correct += (predictions == y).sum().item()\n",
    "                total += X.size(0)\n",
    "                \n",
    "                tp += ((predictions == 1) & (y == 1)).sum().item()\n",
    "                fp += ((predictions == 1) & (y == 0)).sum().item()\n",
    "                tn += ((predictions == 0) & (y == 0)).sum().item()\n",
    "                fn += ((predictions == 0) & (y == 1)).sum().item()\n",
    "        \n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        return float(val_loss), total, {\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"threshold\": float(optimal_thresh)\n",
    "        }\n",
    "\n",
    "def start_compatible_enhanced_client(client_id=0, server_address=None):\n",
    "    \"\"\"Start compatible enhanced client\"\"\"\n",
    "    if server_address:\n",
    "        CLIENT_CONFIG[\"server_address\"] = server_address\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    data_path = f\"framingham_part{client_id+1}.csv\"\n",
    "    if not os.path.exists(data_path):\n",
    "        logger.error(f\"Data file {data_path} not found\")\n",
    "        return\n",
    "    \n",
    "    train_dataloader, val_dataloader, input_size, class_weights = load_enhanced_balanced_data(data_path)\n",
    "    \n",
    "    # Create compatible model (SAME architecture as original server)\n",
    "    model = HeartDiseaseModel(input_size=15).to(device)  # Force to 15\n",
    "    logger.info(f\"Compatible enhanced model initialized: input_size={input_size}\")\n",
    "    \n",
    "    # Create client\n",
    "    client = CompatibleEnhancedClient(model, train_dataloader, val_dataloader, device, class_weights, client_id)\n",
    "    \n",
    "    print(f\"\\n🚀 COMPATIBLE ENHANCED FL CLIENT {client_id}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"✅ SAME architecture as your original server\")\n",
    "    print(\"🎯 ENHANCED training techniques and regularization\")\n",
    "    print(\"📊 Adaptive threshold optimization\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Server: {CLIENT_CONFIG['server_address']}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "    print(f\"\\nEnhancements:\")\n",
    "    print(\"   📈 Better regularization (dropout, weight decay, gradient clipping)\")\n",
    "    print(\"   🎯 Adaptive threshold finding\")\n",
    "    print(\"   ⚖️ More conservative class balancing\")\n",
    "    print(\"   🏥 Validation-based optimization\")\n",
    "    print(\"   🛡️ Label smoothing and enhanced focal loss\")\n",
    "    print(\"\\n🔌 Connecting to your existing server...\")\n",
    "    \n",
    "    fl.client.start_client(server_address=CLIENT_CONFIG[\"server_address\"], client=client)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_compatible_enhanced_client(client_id=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
