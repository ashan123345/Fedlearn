{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb15c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FedAvg.__init__() got an unexpected keyword argument 'on_client_connected'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m server_state \u001b[38;5;241m=\u001b[39m FLServer(min_clients\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# --- Define the FedProx strategy with client connect/disconnect hooks ---\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m strategy \u001b[38;5;241m=\u001b[39m \u001b[43mFedProxStrategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfraction_fit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_fit_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_available_clients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_client_connected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_connected\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_client_disconnected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_disconnected\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# --- Run server in a background thread ---\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_server\u001b[39m():\n",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m, in \u001b[0;36mFedProxStrategy.__init__\u001b[1;34m(self, mu, **kwargs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu \u001b[38;5;241m=\u001b[39m mu\n",
      "\u001b[1;31mTypeError\u001b[0m: FedAvg.__init__() got an unexpected keyword argument 'on_client_connected'"
     ]
    }
   ],
   "source": [
    "# Global Model Client - Connects to FedProx Server for Evaluation (Fixed)\n",
    "import flwr as fl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(\"GlobalModelClient\")\n",
    "\n",
    "# Global Model Configuration\n",
    "GLOBAL_MODEL_CONFIG = {\n",
    "    \"dropout_rate\": 0.3,\n",
    "    \"test_data_path\": \"test_data.csv\",  # Default test data file\n",
    "    \"batch_size\": 32,\n",
    "    \"server_address\": \"localhost:8081\",  # Connect to evaluation server\n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "# Heart Disease Model (same as training clients)\n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(GLOBAL_MODEL_CONFIG[\"dropout_rate\"]),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(GLOBAL_MODEL_CONFIG[\"dropout_rate\"]),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def create_synthetic_test_data(num_samples=1000, input_size=15):\n",
    "    \"\"\"Create synthetic test data if no test file is available\"\"\"\n",
    "    print(f\"üîß Creating synthetic test data with {num_samples} samples and {input_size} features\")\n",
    "    \n",
    "    # Generate random features\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    X = np.random.randn(num_samples, input_size)\n",
    "    \n",
    "    # Create synthetic labels with some correlation to features\n",
    "    # Use a simple linear combination with some noise\n",
    "    weights = np.random.randn(input_size) * 0.1\n",
    "    linear_combination = X @ weights\n",
    "    probabilities = 1 / (1 + np.exp(-linear_combination))  # Sigmoid\n",
    "    y = (probabilities > 0.5).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    feature_names = [f\"feature_{i}\" for i in range(input_size)]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df[\"TenYearCHD\"] = y\n",
    "    \n",
    "    print(f\"üìä Synthetic test data class distribution:\")\n",
    "    print(f\"   Negative cases: {(y == 0).sum()} ({(y == 0).mean()*100:.1f}%)\")\n",
    "    print(f\"   Positive cases: {(y == 1).sum()} ({(y == 1).mean()*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_test_data(data_path=None):\n",
    "    \"\"\"Load test data for global model evaluation\"\"\"\n",
    "    if data_path is None:\n",
    "        data_path = GLOBAL_MODEL_CONFIG[\"test_data_path\"]\n",
    "    \n",
    "    try:\n",
    "        # Try to load the specified file\n",
    "        if os.path.exists(data_path):\n",
    "            df = pd.read_csv(data_path)\n",
    "            print(f\"‚úì Loaded test data from {data_path} with shape {df.shape}\")\n",
    "        else:\n",
    "            print(f\"‚ö† Test data file {data_path} not found\")\n",
    "            print(\"üîß Creating synthetic test data for demonstration...\")\n",
    "            df = create_synthetic_test_data(num_samples=1000, input_size=15)\n",
    "        \n",
    "        # Handle missing values\n",
    "        missing_values = df.isnull().sum().sum()\n",
    "        if missing_values > 0:\n",
    "            print(f\"‚ö† Found {missing_values} missing values in test data, dropping rows\")\n",
    "            df.dropna(inplace=True)\n",
    "        \n",
    "        # Check for target column\n",
    "        if \"TenYearCHD\" not in df.columns:\n",
    "            print(\"‚ö† Target column 'TenYearCHD' not found, creating synthetic target\")\n",
    "            # Create a synthetic target based on features\n",
    "            feature_cols = [col for col in df.columns if col != \"TenYearCHD\"]\n",
    "            if len(feature_cols) > 0:\n",
    "                # Simple synthetic target\n",
    "                df[\"TenYearCHD\"] = (df[feature_cols].sum(axis=1) > df[feature_cols].sum(axis=1).median()).astype(int)\n",
    "            else:\n",
    "                raise ValueError(\"No features found to create synthetic target!\")\n",
    "        \n",
    "        # Split features and target\n",
    "        X = df.drop(columns=[\"TenYearCHD\"])\n",
    "        y = df[\"TenYearCHD\"]\n",
    "        \n",
    "        print(f\"üìä Test data class distribution:\")\n",
    "        print(f\"   Negative cases: {(y == 0).sum()} ({(y == 0).mean()*100:.1f}%)\")\n",
    "        print(f\"   Positive cases: {(y == 1).sum()} ({(y == 1).mean()*100:.1f}%)\")\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        # Create dataloader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=GLOBAL_MODEL_CONFIG[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        print(f\"‚úì Created test dataloader with {len(dataset)} samples and {X.shape[1]} features\")\n",
    "        return dataloader, X.shape[1], len(dataset)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading test data: {str(e)}\")\n",
    "        print(\"üîß Falling back to synthetic data...\")\n",
    "        try:\n",
    "            df = create_synthetic_test_data()\n",
    "            X = df.drop(columns=[\"TenYearCHD\"])\n",
    "            y = df[\"TenYearCHD\"]\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "            y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "            \n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            dataloader = DataLoader(dataset, batch_size=GLOBAL_MODEL_CONFIG[\"batch_size\"], shuffle=False)\n",
    "            \n",
    "            return dataloader, X.shape[1], len(dataset)\n",
    "        except Exception as e2:\n",
    "            logger.error(f\"Failed to create synthetic data: {str(e2)}\")\n",
    "            return None, None, None\n",
    "\n",
    "class GlobalModelClient(fl.client.NumPyClient):\n",
    "    \"\"\"Global Model Client that evaluates aggregated models on test data\"\"\"\n",
    "    \n",
    "    def __init__(self, model, test_dataloader, device, num_test_samples):\n",
    "        self.model = model\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.device = device\n",
    "        self.num_test_samples = num_test_samples\n",
    "        self.round_count = 0\n",
    "        \n",
    "        print(f\"üß† Global Model Client initialized\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(f\"   Test samples: {num_test_samples}\")\n",
    "        print(f\"   Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Return current model parameters (not used for global model)\"\"\"\n",
    "        # Global model doesn't need to send parameters back in evaluation-only mode\n",
    "        return [val.cpu().detach().numpy() for val in self.model.parameters()]\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"Set model parameters from server (aggregated from training clients)\"\"\"\n",
    "        try:\n",
    "            # Convert parameters to model state dict\n",
    "            params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "            state_dict = {k: torch.tensor(v, device=self.device) for k, v in params_dict}\n",
    "            self.model.load_state_dict(state_dict, strict=True)\n",
    "            print(f\"‚úì Global Model updated with aggregated parameters\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error updating Global Model parameters: {str(e)}\")\n",
    "            print(f\"   Parameter shapes received: {[p.shape for p in parameters]}\")\n",
    "            print(f\"   Model expects: {[p.shape for p in self.model.parameters()]}\")\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Global model doesn't participate in training\"\"\"\n",
    "        # Global model is evaluation-only, so we don't train\n",
    "        # Just update parameters and return them unchanged\n",
    "        self.set_parameters(parameters)\n",
    "        \n",
    "        server_round = config.get(\"server_round\", 0)\n",
    "        print(f\"üß† Global Model received updated parameters for Round {server_round}\")\n",
    "        print(f\"   (Global Model does not participate in training)\")\n",
    "        \n",
    "        # Return unchanged parameters since we don't train\n",
    "        return self.get_parameters(config), 0, {}\n",
    "    \n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluate the aggregated model on test data\"\"\"\n",
    "        server_round = config.get(\"server_round\", 0)\n",
    "        self.round_count = server_round\n",
    "        \n",
    "        print(f\"\\nüîç Global Model Evaluation - Round {server_round}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Update model with aggregated parameters from server\n",
    "        self.set_parameters(parameters)\n",
    "        \n",
    "        # Set device and evaluation mode\n",
    "        device = torch.device(self.device)\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        criterion = nn.BCELoss()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Store predictions for detailed metrics\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        all_labels = []\n",
    "        \n",
    "        print(\"üìä Evaluating on test data...\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.test_dataloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Store predictions and labels\n",
    "                predictions = (outputs > 0.5).float()\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(targets.cpu().numpy())\n",
    "                \n",
    "                # Update metrics\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "                total += targets.size(0)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        avg_loss = test_loss / total if total > 0 else 0.0\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        all_predictions = np.array(all_predictions).flatten()\n",
    "        all_probabilities = np.array(all_probabilities).flatten()\n",
    "        all_labels = np.array(all_labels).flatten()\n",
    "        \n",
    "        # AUC Score\n",
    "        try:\n",
    "            auc_score = roc_auc_score(all_labels, all_probabilities)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Could not calculate AUC: {str(e)}\")\n",
    "            auc_score = 0.0\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        try:\n",
    "            cm = confusion_matrix(all_labels, all_predictions)\n",
    "            if cm.size == 4:\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                # Handle edge cases where confusion matrix doesn't have 4 elements\n",
    "                tn = fp = fn = tp = 0\n",
    "                if cm.size == 1:\n",
    "                    if all_labels[0] == 0 and all_predictions[0] == 0:\n",
    "                        tn = cm[0, 0]\n",
    "                    elif all_labels[0] == 1 and all_predictions[0] == 1:\n",
    "                        tp = cm[0, 0]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Could not calculate confusion matrix: {str(e)}\")\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        \n",
    "        # Display detailed results\n",
    "        print(f\"\\nüìà Round {server_round} - Global Model Evaluation Results:\")\n",
    "        print(f\"   üî¥ Loss: {avg_loss:.4f}\")\n",
    "        print(f\"   üü¢ Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "        print(f\"   üîµ AUC: {auc_score:.4f}\")\n",
    "        print(f\"   üìä Precision: {precision:.4f}\")\n",
    "        print(f\"   üìä Recall (Sensitivity): {recall:.4f}\")\n",
    "        print(f\"   üìä Specificity: {specificity:.4f}\")\n",
    "        print(f\"   üìä F1-Score: {f1_score:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìã Confusion Matrix:\")\n",
    "        print(f\"   True Negatives:  {tn}\")\n",
    "        print(f\"   False Positives: {fp}\")\n",
    "        print(f\"   False Negatives: {fn}\")\n",
    "        print(f\"   True Positives:  {tp}\")\n",
    "        \n",
    "        # Performance interpretation\n",
    "        if server_round > 1:\n",
    "            print(f\"\\nüí° Performance Interpretation:\")\n",
    "            if accuracy > 0.85:\n",
    "                print(f\"   üéØ Excellent performance!\")\n",
    "            elif accuracy > 0.75:\n",
    "                print(f\"   ‚úÖ Good performance\")\n",
    "            elif accuracy > 0.65:\n",
    "                print(f\"   ‚ö† Moderate performance\")\n",
    "            else:\n",
    "                print(f\"   üîß Needs improvement\")\n",
    "            \n",
    "            if auc_score > 0.8:\n",
    "                print(f\"   üèÜ Strong discriminative ability (AUC > 0.8)\")\n",
    "            elif auc_score > 0.7:\n",
    "                print(f\"   ‚úÖ Good discriminative ability\")\n",
    "            else:\n",
    "                print(f\"   ‚ö† Limited discriminative ability\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Return metrics to server\n",
    "        metrics = {\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"auc\": float(auc_score),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1_score\": float(f1_score),\n",
    "            \"specificity\": float(specificity),\n",
    "            \"true_positives\": int(tp),\n",
    "            \"true_negatives\": int(tn),\n",
    "            \"false_positives\": int(fp),\n",
    "            \"false_negatives\": int(fn)\n",
    "        }\n",
    "        \n",
    "        return float(avg_loss), self.num_test_samples, metrics\n",
    "\n",
    "def start_global_model_client(server_address=None, test_data_path=None):\n",
    "    \"\"\"Start the Global Model client\"\"\"\n",
    "    \n",
    "    print(\"üß† Starting Global Model Client\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Update configuration\n",
    "    if server_address:\n",
    "        GLOBAL_MODEL_CONFIG[\"server_address\"] = server_address\n",
    "    if test_data_path:\n",
    "        GLOBAL_MODEL_CONFIG[\"test_data_path\"] = test_data_path\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    GLOBAL_MODEL_CONFIG[\"device\"] = device.type\n",
    "    print(f\"üîß Using device: {device}\")\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"üìÇ Loading test data...\")\n",
    "    test_dataloader, input_size, num_test_samples = load_test_data()\n",
    "    \n",
    "    if test_dataloader is None:\n",
    "        print(\"‚ùå Failed to load test data. Cannot start Global Model.\")\n",
    "        return False\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"üèóÔ∏è Initializing Global Model...\")\n",
    "    model = HeartDiseaseModel(input_size=input_size).to(device)\n",
    "    \n",
    "    # Create Global Model client\n",
    "    client = GlobalModelClient(model, test_dataloader, device, num_test_samples)\n",
    "    \n",
    "    print(f\"\\nüåê Connecting to server at {GLOBAL_MODEL_CONFIG['server_address']}\")\n",
    "    print(\"üéØ Global Model will:\")\n",
    "    print(\"   1. Connect to evaluation server\")\n",
    "    print(\"   2. Receive aggregated models after each round\")\n",
    "    print(\"   3. Evaluate on test data\")\n",
    "    print(\"   4. Report detailed metrics back to server\")\n",
    "    print(\"\\n‚è≥ Connecting...\")\n",
    "    \n",
    "    try:\n",
    "        # Start the client (this will block until training is complete)\n",
    "        fl.client.start_client(\n",
    "            server_address=GLOBAL_MODEL_CONFIG[\"server_address\"], \n",
    "            client=client\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Global Model client completed successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö† Global Model client interrupted by user\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Global Model client error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Quick start function\n",
    "def start_global_model():\n",
    "    \"\"\"Quick start Global Model with default settings\"\"\"\n",
    "    print(\"üß† Quick Start - Global Model Client\")\n",
    "    return start_global_model_client(\n",
    "        server_address=\"localhost:8081\",  # Connect to evaluation server\n",
    "        test_data_path=\"test_data.csv\"\n",
    "    )\n",
    "\n",
    "def start_global_model_with_synthetic_data():\n",
    "    \"\"\"Start Global Model with synthetic test data\"\"\"\n",
    "    print(\"üß† Quick Start - Global Model Client (Synthetic Data)\")\n",
    "    return start_global_model_client(\n",
    "        server_address=\"localhost:8081\",\n",
    "        test_data_path=\"nonexistent.csv\"  # This will trigger synthetic data creation\n",
    "    )\n",
    "\n",
    "print(\"üß† Global Model Client Ready!\")\n",
    "print(\"\\nüìù Available Commands:\")\n",
    "print(\"   start_global_model()                    - Quick start with defaults\")\n",
    "print(\"   start_global_model_with_synthetic_data() - Quick start with synthetic test data\")\n",
    "print(\"   start_global_model_client()             - Start with custom parameters\")\n",
    "print(\"\\nüí° Usage Instructions:\")\n",
    "print(\"   1. Make sure the evaluation server is running on port 8081\")\n",
    "print(\"   2. Run start_global_model() to connect\")\n",
    "print(\"   3. Global Model will evaluate the current model parameters\")\n",
    "print(\"   4. If no test data file exists, synthetic data will be created\")\n",
    "print(\"\\nüöÄ To start: run start_global_model()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debfc39f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_global_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstart_global_model\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start_global_model' is not defined"
     ]
    }
   ],
   "source": [
    "start_global_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c25a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'TypeAliasType' could not be imported from 'c:\\Users\\Ashan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\typing_extensions.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Global Model Client - Connects to FedProx Server for Evaluation\n",
    "import flwr as fl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "logger = logging.getLogger(\"GlobalModelClient\")\n",
    "\n",
    "# Global Model Configuration\n",
    "GLOBAL_MODEL_CONFIG = {\n",
    "    \"dropout_rate\": 0.3,\n",
    "    \"test_data_path\": \"unseendata.csv\",  # Test on unseen data\n",
    "    \"batch_size\": 32,\n",
    "    \"server_address\": \"localhost:8080\",  # Match your training client's server address\n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "# Heart Disease Model (same as your training clients)\n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(GLOBAL_MODEL_CONFIG[\"dropout_rate\"]),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(GLOBAL_MODEL_CONFIG[\"dropout_rate\"]),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def create_synthetic_test_data(num_samples=1000, input_size=15):\n",
    "    \"\"\"Create synthetic test data if no test file is available\"\"\"\n",
    "    print(f\"üîß Creating synthetic test data with {num_samples} samples and {input_size} features\")\n",
    "    \n",
    "    # Generate random features\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    X = np.random.randn(num_samples, input_size)\n",
    "    \n",
    "    # Create synthetic labels with some correlation to features\n",
    "    weights = np.random.randn(input_size) * 0.1\n",
    "    linear_combination = X @ weights\n",
    "    probabilities = 1 / (1 + np.exp(-linear_combination))  # Sigmoid\n",
    "    y = (probabilities > 0.5).astype(int)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    feature_names = [f\"feature_{i}\" for i in range(input_size)]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df[\"TenYearCHD\"] = y\n",
    "    \n",
    "    print(f\"üìä Synthetic test data class distribution:\")\n",
    "    print(f\"   Negative cases: {(y == 0).sum()} ({(y == 0).mean()*100:.1f}%)\")\n",
    "    print(f\"   Positive cases: {(y == 1).sum()} ({(y == 1).mean()*100:.1f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_test_data(data_path=None):\n",
    "    \"\"\"Load test data for global model evaluation\"\"\"\n",
    "    if data_path is None:\n",
    "        data_path = GLOBAL_MODEL_CONFIG[\"test_data_path\"]\n",
    "    \n",
    "    try:\n",
    "        # Try to load the specified file\n",
    "        if os.path.exists(data_path):\n",
    "            df = pd.read_csv(data_path)\n",
    "            print(f\"‚úì Loaded test data from {data_path} with shape {df.shape}\")\n",
    "        else:\n",
    "            print(f\"‚ö† Test data file {data_path} not found\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        missing_values = df.isnull().sum().sum()\n",
    "        if missing_values > 0:\n",
    "            print(f\"‚ö† Found {missing_values} missing values in test data, dropping rows\")\n",
    "            df.dropna(inplace=True)\n",
    "        \n",
    "        # Check for target column\n",
    "        if \"TenYearCHD\" not in df.columns:\n",
    "            print(\"‚ö† Target column 'TenYearCHD' not found, creating synthetic target\")\n",
    "            # Create a synthetic target based on features\n",
    "            feature_cols = [col for col in df.columns if col != \"TenYearCHD\"]\n",
    "            if len(feature_cols) > 0:\n",
    "                # Simple synthetic target\n",
    "                df[\"TenYearCHD\"] = (df[feature_cols].sum(axis=1) > df[feature_cols].sum(axis=1).median()).astype(int)\n",
    "            else:\n",
    "                raise ValueError(\"No features found to create synthetic target!\")\n",
    "        \n",
    "        # Split features and target\n",
    "        X = df.drop(columns=[\"TenYearCHD\"])\n",
    "        y = df[\"TenYearCHD\"]\n",
    "        \n",
    "        print(f\"üìä Test data (unseendata.csv) class distribution:\")\n",
    "        print(f\"   Negative cases: {(y == 0).sum()} ({(y == 0).mean()*100:.1f}%)\")\n",
    "        print(f\"   Positive cases: {(y == 1).sum()} ({(y == 1).mean()*100:.1f}%)\")\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        # Create dataloader\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=GLOBAL_MODEL_CONFIG[\"batch_size\"], shuffle=False)\n",
    "        \n",
    "        print(f\"‚úì Created test dataloader with {len(dataset)} samples and {X.shape[1]} features\")\n",
    "        return dataloader, X.shape[1], len(dataset)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading unseendata.csv: {str(e)}\")\n",
    "        print(\"üîß Falling back to synthetic data...\")\n",
    "        try:\n",
    "            df = create_synthetic_test_data()\n",
    "            X = df.drop(columns=[\"TenYearCHD\"])\n",
    "            y = df[\"TenYearCHD\"]\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            \n",
    "            X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "            y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
    "            \n",
    "            dataset = TensorDataset(X_tensor, y_tensor)\n",
    "            dataloader = DataLoader(dataset, batch_size=GLOBAL_MODEL_CONFIG[\"batch_size\"], shuffle=False)\n",
    "            \n",
    "            return dataloader, X.shape[1], len(dataset)\n",
    "        except Exception as e2:\n",
    "            logger.error(f\"Failed to create synthetic data: {str(e2)}\")\n",
    "            return None, None, None\n",
    "\n",
    "class GlobalModelClient(fl.client.NumPyClient):\n",
    "    \"\"\"Global Model Client that evaluates aggregated models on unseen test data\"\"\"\n",
    "    \n",
    "    def __init__(self, model, test_dataloader, device, num_test_samples):\n",
    "        self.model = model\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.device = device\n",
    "        self.num_test_samples = num_test_samples\n",
    "        self.round_count = 0\n",
    "        \n",
    "        print(f\"üß† Global Model Client initialized\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(f\"   Test samples (unseendata.csv): {num_test_samples}\")\n",
    "        print(f\"   Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Return current model parameters (not used for global model)\"\"\"\n",
    "        # Global model doesn't need to send parameters back in evaluation-only mode\n",
    "        return [val.cpu().detach().numpy() for val in self.model.parameters()]\n",
    "    \n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"Set model parameters from server (aggregated from training clients)\"\"\"\n",
    "        try:\n",
    "            # Convert parameters to model state dict\n",
    "            params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "            state_dict = {k: torch.tensor(v, device=self.device) for k, v in params_dict}\n",
    "            self.model.load_state_dict(state_dict, strict=True)\n",
    "            print(f\"‚úì Global Model updated with aggregated parameters\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error updating Global Model parameters: {str(e)}\")\n",
    "            print(f\"   Parameter shapes received: {[p.shape for p in parameters]}\")\n",
    "            print(f\"   Model expects: {[p.shape for p in self.model.parameters()]}\")\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Global model doesn't participate in training\"\"\"\n",
    "        # Global model is evaluation-only, so we don't train\n",
    "        # Just update parameters and return them unchanged\n",
    "        self.set_parameters(parameters)\n",
    "        \n",
    "        server_round = config.get(\"server_round\", 0)\n",
    "        print(f\"üß† Global Model received updated parameters for Round {server_round}\")\n",
    "        print(f\"   (Global Model does not participate in training)\")\n",
    "        \n",
    "        # Return unchanged parameters since we don't train\n",
    "        return self.get_parameters(config), 0, {}\n",
    "    \n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluate the aggregated model on unseen test data\"\"\"\n",
    "        server_round = config.get(\"server_round\", 0)\n",
    "        self.round_count = server_round\n",
    "        \n",
    "        print(f\"\\nüîç Global Model Evaluation - Round {server_round}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"üìã Testing on unseendata.csv (Completely Unseen Data)\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Update model with aggregated parameters from server\n",
    "        self.set_parameters(parameters)\n",
    "        \n",
    "        # Set device and evaluation mode\n",
    "        device = torch.device(self.device)\n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Evaluation metrics\n",
    "        criterion = nn.BCELoss()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Store predictions for detailed metrics\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "        all_labels = []\n",
    "        \n",
    "        print(\"üìä Evaluating on unseen test data...\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.test_dataloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                # Store predictions and labels\n",
    "                predictions = (outputs > 0.5).float()\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(outputs.cpu().numpy())\n",
    "                all_labels.extend(targets.cpu().numpy())\n",
    "                \n",
    "                # Update metrics\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "                total += targets.size(0)\n",
    "                correct += (predictions == targets).sum().item()\n",
    "        \n",
    "        # Calculate final metrics\n",
    "        avg_loss = test_loss / total if total > 0 else 0.0\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        all_predictions = np.array(all_predictions).flatten()\n",
    "        all_probabilities = np.array(all_probabilities).flatten()\n",
    "        all_labels = np.array(all_labels).flatten()\n",
    "        \n",
    "        # AUC Score\n",
    "        try:\n",
    "            auc_score = roc_auc_score(all_labels, all_probabilities)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Could not calculate AUC: {str(e)}\")\n",
    "            auc_score = 0.0\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        try:\n",
    "            cm = confusion_matrix(all_labels, all_predictions)\n",
    "            if cm.size == 4:\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                # Handle edge cases where confusion matrix doesn't have 4 elements\n",
    "                tn = fp = fn = tp = 0\n",
    "                if cm.size == 1:\n",
    "                    if all_labels[0] == 0 and all_predictions[0] == 0:\n",
    "                        tn = cm[0, 0]\n",
    "                    elif all_labels[0] == 1 and all_predictions[0] == 1:\n",
    "                        tp = cm[0, 0]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Could not calculate confusion matrix: {str(e)}\")\n",
    "            tn = fp = fn = tp = 0\n",
    "        \n",
    "        # Additional metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        \n",
    "        # Display detailed results\n",
    "        print(f\"\\nüìà Round {server_round} - Global Model Evaluation on Unseen Data:\")\n",
    "        print(f\"   üî¥ Loss: {avg_loss:.4f}\")\n",
    "        print(f\"   üü¢ Accuracy: {accuracy:.4f} ({correct}/{total})\")\n",
    "        print(f\"   üîµ AUC: {auc_score:.4f}\")\n",
    "        print(f\"   üìä Precision: {precision:.4f}\")\n",
    "        print(f\"   üìä Recall (Sensitivity): {recall:.4f}\")\n",
    "        print(f\"   üìä Specificity: {specificity:.4f}\")\n",
    "        print(f\"   üìä F1-Score: {f1_score:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìã Confusion Matrix (Unseen Data):\")\n",
    "        print(f\"   True Negatives:  {tn}\")\n",
    "        print(f\"   False Positives: {fp}\")\n",
    "        print(f\"   False Negatives: {fn}\")\n",
    "        print(f\"   True Positives:  {tp}\")\n",
    "        \n",
    "        # Performance interpretation\n",
    "        if server_round > 1:\n",
    "            print(f\"\\nüí° Performance on Unseen Data:\")\n",
    "            if accuracy > 0.85:\n",
    "                print(f\"   üéØ Excellent generalization!\")\n",
    "            elif accuracy > 0.75:\n",
    "                print(f\"   ‚úÖ Good generalization\")\n",
    "            elif accuracy > 0.65:\n",
    "                print(f\"   ‚ö† Moderate generalization\")\n",
    "            else:\n",
    "                print(f\"   üîß Poor generalization - may be overfitting\")\n",
    "            \n",
    "            if auc_score > 0.8:\n",
    "                print(f\"   üèÜ Strong discriminative ability on unseen data (AUC > 0.8)\")\n",
    "            elif auc_score > 0.7:\n",
    "                print(f\"   ‚úÖ Good discriminative ability on unseen data\")\n",
    "            else:\n",
    "                print(f\"   ‚ö† Limited discriminative ability on unseen data\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Return metrics to server\n",
    "        metrics = {\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"auc\": float(auc_score),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1_score\": float(f1_score),\n",
    "            \"specificity\": float(specificity),\n",
    "            \"true_positives\": int(tp),\n",
    "            \"true_negatives\": int(tn),\n",
    "            \"false_positives\": int(fp),\n",
    "            \"false_negatives\": int(fn)\n",
    "        }\n",
    "        \n",
    "        return float(avg_loss), self.num_test_samples, metrics\n",
    "\n",
    "def start_global_model_client(server_address=None, test_data_path=None):\n",
    "    \"\"\"Start the Global Model client\"\"\"\n",
    "    \n",
    "    print(\"üß† Starting Global Model Client\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"üìã Testing on UNSEEN DATA (unseendata.csv)\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Update configuration\n",
    "    if server_address:\n",
    "        GLOBAL_MODEL_CONFIG[\"server_address\"] = server_address\n",
    "    if test_data_path:\n",
    "        GLOBAL_MODEL_CONFIG[\"test_data_path\"] = test_data_path\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    GLOBAL_MODEL_CONFIG[\"device\"] = device.type\n",
    "    print(f\"üîß Using device: {device}\")\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"üìÇ Loading unseen test data...\")\n",
    "    test_dataloader, input_size, num_test_samples = load_test_data()\n",
    "    \n",
    "    if test_dataloader is None:\n",
    "        print(\"‚ùå Failed to load unseen test data. Cannot start Global Model.\")\n",
    "        return False\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"üèóÔ∏è Initializing Global Model...\")\n",
    "    model = HeartDiseaseModel(input_size=input_size).to(device)\n",
    "    \n",
    "    # Create Global Model client\n",
    "    client = GlobalModelClient(model, test_dataloader, device, num_test_samples)\n",
    "    \n",
    "    print(f\"\\nüåê Connecting to server at {GLOBAL_MODEL_CONFIG['server_address']}\")\n",
    "    print(\"üéØ Global Model will:\")\n",
    "    print(\"   1. Connect to evaluation server\")\n",
    "    print(\"   2. Receive aggregated models after each round\")\n",
    "    print(\"   3. Evaluate on UNSEEN data (unseendata.csv)\")\n",
    "    print(\"   4. Report detailed metrics back to server\")\n",
    "    print(\"\\n‚è≥ Connecting...\")\n",
    "    \n",
    "    try:\n",
    "        # Start the client (this will block until training is complete)\n",
    "        fl.client.start_client(\n",
    "            server_address=GLOBAL_MODEL_CONFIG[\"server_address\"], \n",
    "            client=client\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ Global Model client completed successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö† Global Model client interrupted by user\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Global Model client error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Quick start function\n",
    "def start_global_model():\n",
    "    \"\"\"Quick start Global Model with default settings\"\"\"\n",
    "    print(\"üß† Quick Start - Global Model Client\")\n",
    "    print(\"üìã Will test on unseendata.csv\")\n",
    "    return start_global_model_client(\n",
    "        server_address=\"localhost:8080\",  # Match your training client\n",
    "        test_data_path=\"unseendata.csv\"\n",
    "    )\n",
    "\n",
    "def start_global_model_with_synthetic_data():\n",
    "    \"\"\"Start Global Model with synthetic test data\"\"\"\n",
    "    print(\"üß† Quick Start - Global Model Client (Synthetic Data)\")\n",
    "    return start_global_model_client(\n",
    "        server_address=\"localhost:8081\",\n",
    "        test_data_path=\"nonexistent.csv\"  # This will trigger synthetic data creation\n",
    "    )\n",
    "\n",
    "# Auto-start when cell is executed\n",
    "print(\"üß† Starting Global Model Client automatically...\")\n",
    "print(\"üìã Will test on unseendata.csv\")\n",
    "\n",
    "# Check if server is available before starting\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def check_server_connection(host=\"localhost\", port=8080, timeout=5):\n",
    "    \"\"\"Check if server is available\"\"\"\n",
    "    try:\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.settimeout(timeout)\n",
    "        result = sock.connect_ex((host, port))\n",
    "        sock.close()\n",
    "        return result == 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Wait for server to be available\n",
    "print(\"üîç Checking server availability...\")\n",
    "max_attempts = 30\n",
    "attempt = 0\n",
    "\n",
    "while attempt < max_attempts:\n",
    "    if check_server_connection():\n",
    "        print(\"‚úÖ Server is available!\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"‚è≥ Waiting for server... (attempt {attempt + 1}/{max_attempts})\")\n",
    "        time.sleep(2)\n",
    "        attempt += 1\n",
    "\n",
    "if attempt >= max_attempts:\n",
    "    print(\"‚ùå Server not available after 60 seconds\")\n",
    "    print(\"üí° Make sure to:\")\n",
    "    print(\"   1. Start the FedProx server first\")\n",
    "    print(\"   2. Wait for it to show 'Waiting for clients to connect...'\")\n",
    "    print(\"   3. Then run this Global Model Client\")\n",
    "else:\n",
    "    # Server is available, start the client\n",
    "    success = start_global_model()\n",
    "    \n",
    "    if success:\n",
    "        print(\"‚úÖ Global Model Client completed successfully!\")\n",
    "    else:\n",
    "        print(\"‚ùå Global Model Client failed to complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
