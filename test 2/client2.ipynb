{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import flwr as fl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ✅ Define the MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc5 = nn.Linear(128, 2)  # Output size = 2 (Binary Classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.relu2(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.relu3(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.relu4(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# ✅ Load Dataset and Preprocess\n",
    "def load_dataset(client_id):\n",
    "    df = pd.read_csv(\"middle_aged_adults.csv\")  # Make sure the dataset is present\n",
    "    X = df.drop(columns=['TenYearCHD']).values  # Features\n",
    "    y = df['TenYearCHD'].values  # Labels\n",
    "\n",
    "    # ✅ Apply SMOTE for balancing\n",
    "    smote = SMOTE(random_state=42 + client_id)\n",
    "    X, y = smote.fit_resample(X, y)\n",
    "\n",
    "    # ✅ Split into training and test sets (each client gets different data)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42 + client_id\n",
    "    )\n",
    "\n",
    "    # ✅ Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # ✅ Create DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, X_train_tensor.shape[1]  # Input size for model\n",
    "\n",
    "# ✅ Initialize Client\n",
    "client_id = int(input(\"Enter client ID (1 or 2): \"))  # Manually input ID\n",
    "train_loader, test_loader, input_size = load_dataset(client_id)\n",
    "\n",
    "model = MLP(input_size=input_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    def __init__(self, client_id):\n",
    "        self.client_id = client_id\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        \"\"\"Return current model parameters.\"\"\"\n",
    "        return [val.cpu().detach().numpy() for val in model.state_dict().values()]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"Update model parameters before training.\"\"\"\n",
    "        state_dict = dict(zip(model.state_dict().keys(), parameters))\n",
    "        model.load_state_dict({k: torch.tensor(v) for k, v in state_dict.items()}, strict=False)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Train the model for 10 local epochs and return updated parameters.\"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        model.train()\n",
    "\n",
    "        for epoch in range(10):  # 🔹 Train for 10 epochs before sending update\n",
    "            total_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"📢 [Client {self.client_id}] Epoch {epoch+1}/10, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        print(f\"📤 Client {self.client_id} training complete. Sending updates.\")\n",
    "        return self.get_parameters(config), len(train_loader.dataset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        \"\"\"Evaluate model and return loss & accuracy.\"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        model.eval()\n",
    "\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "                correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        print(f\"✅ Client {self.client_id} Evaluation: Loss={total_loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "        return float(total_loss), len(test_loader.dataset), {\"accuracy\": accuracy}\n",
    "\n",
    "# ✅ Start the client\n",
    "fl.client.start_numpy_client(server_address=\"127.0.0.1:8081\", client=FLClient(client_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ✅ Define the MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc5 = nn.Linear(128, 2)  # Output size = 2 (Binary Classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.relu2(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.relu3(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.relu4(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "# ✅ Load Dataset and Preprocess\n",
    "def load_dataset():\n",
    "    df = pd.read_csv(\"middle_aged_adults.csv\")  # Make sure the dataset is present\n",
    "    X = df.drop(columns=['TenYearCHD']).values  # Features\n",
    "    y = df['TenYearCHD'].values  # Labels\n",
    "\n",
    "    # ✅ Apply SMOTE for balancing\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X, y = smote.fit_resample(X, y)\n",
    "\n",
    "    # ✅ Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # ✅ Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # ✅ Create DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, X_train_tensor.shape[1]\n",
    "\n",
    "# ✅ Initialize Training Components\n",
    "train_loader, test_loader, input_size = load_dataset()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MLP(input_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0008, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ✅ Training Loop (10 epochs)\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"📢 Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ✅ Evaluation\n",
    "model.eval()\n",
    "total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(dim=1) == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0.0\n",
    "print(f\"✅ Final Evaluation: Loss={total_loss:.4f}, Accuracy={accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ✅ Load the three datasets\n",
    "young_adults = pd.read_csv(\"young_adults.csv\")\n",
    "middle_aged_adults = pd.read_csv(\"middle_aged_adults.csv\")\n",
    "older_adults = pd.read_csv(\"older_adults.csv\")\n",
    "\n",
    "# ✅ Separate features and target variable\n",
    "def prepare_data(df):\n",
    "    X = df.drop(columns=[\"TenYearCHD\"])\n",
    "    y = df[\"TenYearCHD\"]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)  # Normalize features\n",
    "    return X_scaled, y, X.columns  # Return feature names for reference\n",
    "\n",
    "X_young, y_young, feature_names = prepare_data(young_adults)\n",
    "X_middle, y_middle, _ = prepare_data(middle_aged_adults)\n",
    "X_older, y_older, _ = prepare_data(older_adults)\n",
    "\n",
    "# ✅ Train Random Forest models\n",
    "def train_rf(X, y):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    return model.feature_importances_\n",
    "\n",
    "importance_young = train_rf(X_young, y_young)\n",
    "importance_middle = train_rf(X_middle, y_middle)\n",
    "importance_older = train_rf(X_older, y_older)\n",
    "\n",
    "# ✅ Combine importance scores into a DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Young Adults\": importance_young,\n",
    "    \"Middle-Aged Adults\": importance_middle,\n",
    "    \"Older Adults\": importance_older\n",
    "}).sort_values(by=\"Middle-Aged Adults\", ascending=False)\n",
    "\n",
    "# ✅ Plot Feature Importance for Comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(feature_names))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, feature_importance_df[\"Young Adults\"], width=width, label=\"Young Adults\")\n",
    "plt.bar(x, feature_importance_df[\"Middle-Aged Adults\"], width=width, label=\"Middle-Aged Adults\")\n",
    "plt.bar(x + width, feature_importance_df[\"Older Adults\"], width=width, label=\"Older Adults\")\n",
    "\n",
    "plt.xticks(ticks=x, labels=feature_importance_df[\"Feature\"], rotation=45, ha=\"right\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.title(\"Feature Importance Comparison Across Age Groups\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load your Framingham dataset\n",
    "df = pd.read_csv(\"framinghamdataset.csv\")  # Change this to your actual dataset path\n",
    "\n",
    "# Drop any missing values (optional, if needed)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"TenYearCHD\"])  # Features (all columns except target)\n",
    "y = df[\"TenYearCHD\"]  # Target variable\n",
    "\n",
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Combine back into a DataFrame\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "df_resampled[\"TenYearCHD\"] = y_resampled\n",
    "\n",
    "# Shuffle the dataset before splitting\n",
    "df_resampled = df_resampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the dataset into 3 random parts for federated clients\n",
    "client_1, client_2, client_3 = np.array_split(df_resampled, 3)\n",
    "\n",
    "# Save each client dataset\n",
    "client_1.to_csv(\"client_1.csv\", index=False)\n",
    "client_2.to_csv(\"client_2.csv\", index=False)\n",
    "client_3.to_csv(\"client_3.csv\", index=False)\n",
    "\n",
    "print(\"✅ Dataset successfully upsampled and randomly split into 3 clients!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load one client dataset (e.g., client_1.csv)\n",
    "df = pd.read_csv(\"client_1.csv\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"TenYearCHD\"]).values  # Features\n",
    "y = df[\"TenYearCHD\"].values  # Target\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)  # Binary classification (2 classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]\n",
    "model = MLP(input_size)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "    \n",
    "    train_acc = correct / total * 100\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "test_acc = correct / total * 100\n",
    "print(f\"\\n🔍 Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Training Model for Client 1...\n",
      "Epoch 10/300, Loss: 16.7775, Train Acc: 70.96%, Val Acc: 74.17%\n",
      "Epoch 20/300, Loss: 15.8957, Train Acc: 72.31%, Val Acc: 72.71%\n",
      "Epoch 30/300, Loss: 15.1266, Train Acc: 74.56%, Val Acc: 71.46%\n",
      "Epoch 40/300, Loss: 14.9917, Train Acc: 76.17%, Val Acc: 72.50%\n",
      "🛑 Early Stopping at Epoch 43. Best Validation Accuracy: 74.38%\n",
      "\n",
      "✅ Client 1 Best Test Accuracy: 74.38%\n",
      "\n",
      "🔹 Training Model for Client 2...\n",
      "Epoch 10/300, Loss: 16.7361, Train Acc: 70.79%, Val Acc: 67.92%\n",
      "Epoch 20/300, Loss: 15.9323, Train Acc: 73.08%, Val Acc: 70.83%\n",
      "Epoch 30/300, Loss: 15.1623, Train Acc: 75.01%, Val Acc: 70.21%\n",
      "Epoch 40/300, Loss: 14.9571, Train Acc: 75.80%, Val Acc: 71.25%\n",
      "Epoch 50/300, Loss: 14.4939, Train Acc: 75.12%, Val Acc: 71.04%\n",
      "🛑 Early Stopping at Epoch 55. Best Validation Accuracy: 71.46%\n",
      "\n",
      "✅ Client 2 Best Test Accuracy: 71.46%\n",
      "\n",
      "🔹 Training Model for Client 3...\n",
      "Epoch 10/300, Loss: 16.5689, Train Acc: 71.67%, Val Acc: 73.75%\n",
      "Epoch 20/300, Loss: 15.3742, Train Acc: 74.54%, Val Acc: 75.42%\n",
      "Epoch 30/300, Loss: 15.2627, Train Acc: 74.80%, Val Acc: 76.67%\n",
      "Epoch 40/300, Loss: 14.2383, Train Acc: 76.63%, Val Acc: 74.38%\n",
      "Epoch 50/300, Loss: 13.7240, Train Acc: 77.46%, Val Acc: 76.88%\n",
      "Epoch 60/300, Loss: 13.6894, Train Acc: 78.20%, Val Acc: 75.21%\n",
      "🛑 Early Stopping at Epoch 70. Best Validation Accuracy: 76.88%\n",
      "\n",
      "✅ Client 3 Best Test Accuracy: 76.88%\n",
      "\n",
      "🎯 Final Accuracies:\n",
      "Client 1: 74.38%\n",
      "Client 2: 71.46%\n",
      "Client 3: 76.88%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load and preprocess dataset\n",
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Feature selection and scaling\n",
    "    X = df.drop(columns=[\"TenYearCHD\"]).values\n",
    "    y = df[\"TenYearCHD\"].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, X_train.shape[1]\n",
    "\n",
    "# Define Improved MLP Model\n",
    "class ImprovedMLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ImprovedMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.fc6 = nn.Linear(64, 2)  # Binary classification\n",
    "\n",
    "        self.swish = nn.SiLU()  # Swish activation instead of ReLU\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.swish(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.swish(self.bn5(self.fc5(x)))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(train_loader, test_loader, input_size, client_id):\n",
    "    print(f\"\\n🔹 Training Model for Client {client_id}...\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ImprovedMLP(input_size).to(device)\n",
    "\n",
    "    # Compute class weights\n",
    "    class_counts = np.bincount([y for _, y in train_loader.dataset])\n",
    "    class_weights = torch.tensor([1.0 / class_counts[0], 1.0 / class_counts[1]], dtype=torch.float32).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # Optimizer & Scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0008, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 300\n",
    "    early_stopping_patience = 20\n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            X_batch, y_batch = batch\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        \n",
    "        train_acc = correct / total * 100\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                X_batch, y_batch = batch\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        val_acc = correct / total * 100\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f\"best_model_client_{client_id}.pth\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(f\"🛑 Early Stopping at Epoch {epoch + 1}. Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    print(f\"\\n✅ Client {client_id} Best Test Accuracy: {best_val_acc:.2f}%\")\n",
    "    return best_val_acc\n",
    "\n",
    "# Train on all three datasets\n",
    "datasets = [\"client_1.csv\", \"client_2.csv\", \"client_3.csv\"]\n",
    "accuracies = {}\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    train_loader, test_loader, input_size = load_data(dataset)\n",
    "    accuracies[f\"Client {i+1}\"] = train_model(train_loader, test_loader, input_size, i+1)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n🎯 Final Accuracies:\")\n",
    "for client, acc in accuracies.items():\n",
    "    print(f\"{client}: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ranger-adabelief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can you reduce the ephos to like 200 700 is too much \n",
    "🔹 Training Model for Client 1...\n",
    "Epoch 10/500, Loss: 33.4631, Train Acc: 74.82%, Val Acc: 71.88%\n",
    "Epoch 20/500, Loss: 31.3419, Train Acc: 77.89%, Val Acc: 72.08%\n",
    "Epoch 30/500, Loss: 30.5089, Train Acc: 78.42%, Val Acc: 72.92%\n",
    "Epoch 40/500, Loss: 29.2711, Train Acc: 79.82%, Val Acc: 72.71%\n",
    "Epoch 50/500, Loss: 28.1338, Train Acc: 82.27%, Val Acc: 73.12%\n",
    "Epoch 60/500, Loss: 28.0618, Train Acc: 81.86%, Val Acc: 71.88%\n",
    "Epoch 70/500, Loss: 27.0236, Train Acc: 82.79%, Val Acc: 73.33%\n",
    "Epoch 80/500, Loss: 26.1234, Train Acc: 84.72%, Val Acc: 73.12%\n",
    "Epoch 90/500, Loss: 25.6369, Train Acc: 84.57%, Val Acc: 73.12%\n",
    "Epoch 100/500, Loss: 24.6136, Train Acc: 86.44%, Val Acc: 73.33%\n",
    "Epoch 110/500, Loss: 24.8100, Train Acc: 85.82%, Val Acc: 72.71%\n",
    "Epoch 120/500, Loss: 24.2571, Train Acc: 87.49%, Val Acc: 75.62%\n",
    "Epoch 130/500, Loss: 23.5207, Train Acc: 87.80%, Val Acc: 73.54%\n",
    "Epoch 140/500, Loss: 23.3109, Train Acc: 87.80%, Val Acc: 74.17%\n",
    "Epoch 150/500, Loss: 23.5639, Train Acc: 86.97%, Val Acc: 74.79%\n",
    "Epoch 160/500, Loss: 22.9443, Train Acc: 88.32%, Val Acc: 73.96%\n",
    "Epoch 170/500, Loss: 22.4119, Train Acc: 89.10%, Val Acc: 75.21%\n",
    "Epoch 180/500, Loss: 22.4375, Train Acc: 88.69%, Val Acc: 73.75%\n",
    "Epoch 190/500, Loss: 21.8623, Train Acc: 90.82%, Val Acc: 75.62%\n",
    "Epoch 200/500, Loss: 21.0324, Train Acc: 90.77%, Val Acc: 74.58%\n",
    "Epoch 210/500, Loss: 21.3956, Train Acc: 89.73%, Val Acc: 77.08%\n",
    "Epoch 220/500, Loss: 20.4825, Train Acc: 91.92%, Val Acc: 76.25%\n",
    "Epoch 230/500, Loss: 21.1599, Train Acc: 90.56%, Val Acc: 76.04%\n",
    "Epoch 240/500, Loss: 20.7430, Train Acc: 91.97%, Val Acc: 76.25%\n",
    "Epoch 250/500, Loss: 20.5461, Train Acc: 91.14%, Val Acc: 76.88%\n",
    "Epoch 260/500, Loss: 20.5796, Train Acc: 91.40%, Val Acc: 76.88%\n",
    "Epoch 270/500, Loss: 20.3981, Train Acc: 91.55%, Val Acc: 76.25%\n",
    "Epoch 280/500, Loss: 20.5405, Train Acc: 91.71%, Val Acc: 78.12%\n",
    "🛑 Early Stopping at Epoch 283. Best Validation Accuracy: 78.75%\n",
    "\n",
    "✅ Client 1 Best Test Accuracy: 78.75%\n",
    "\n",
    "🔹 Training Model for Client 2...\n",
    "Epoch 10/500, Loss: 33.9601, Train Acc: 72.56%, Val Acc: 69.79%\n",
    "Epoch 20/500, Loss: 31.9448, Train Acc: 76.73%, Val Acc: 72.08%\n",
    "Epoch 30/500, Loss: 31.3564, Train Acc: 76.89%, Val Acc: 73.54%\n",
    "Epoch 40/500, Loss: 30.0252, Train Acc: 79.03%, Val Acc: 72.29%\n",
    "Epoch 50/500, Loss: 29.3394, Train Acc: 79.50%, Val Acc: 72.08%\n",
    "Epoch 60/500, Loss: 27.7111, Train Acc: 82.00%, Val Acc: 72.71%\n",
    "Epoch 70/500, Loss: 26.9407, Train Acc: 84.14%, Val Acc: 73.75%\n",
    "Epoch 80/500, Loss: 26.7294, Train Acc: 83.20%, Val Acc: 72.71%\n",
    "Epoch 90/500, Loss: 26.1666, Train Acc: 83.88%, Val Acc: 73.96%\n",
    "Epoch 100/500, Loss: 25.6589, Train Acc: 85.55%, Val Acc: 73.96%\n",
    "Epoch 110/500, Loss: 25.5070, Train Acc: 84.82%, Val Acc: 74.17%\n",
    "Epoch 120/500, Loss: 25.0719, Train Acc: 86.54%, Val Acc: 75.62%\n",
    "Epoch 130/500, Loss: 24.2863, Train Acc: 86.12%, Val Acc: 75.42%\n",
    "Epoch 140/500, Loss: 23.8278, Train Acc: 87.85%, Val Acc: 76.04%\n",
    "Epoch 150/500, Loss: 23.2772, Train Acc: 88.78%, Val Acc: 76.25%\n",
    "Epoch 160/500, Loss: 23.4531, Train Acc: 87.95%, Val Acc: 75.42%\n",
    "Epoch 170/500, Loss: 22.3687, Train Acc: 89.57%, Val Acc: 76.88%\n",
    "Epoch 180/500, Loss: 22.5469, Train Acc: 88.47%, Val Acc: 74.38%\n",
    "Epoch 190/500, Loss: 22.3148, Train Acc: 89.31%, Val Acc: 75.21%\n",
    "Epoch 200/500, Loss: 22.4409, Train Acc: 88.94%, Val Acc: 76.46%\n",
    "Epoch 210/500, Loss: 21.8960, Train Acc: 89.62%, Val Acc: 77.50%\n",
    "Epoch 220/500, Loss: 21.9468, Train Acc: 90.14%, Val Acc: 76.88%\n",
    "Epoch 230/500, Loss: 21.6842, Train Acc: 90.71%, Val Acc: 76.67%\n",
    "Epoch 240/500, Loss: 20.9450, Train Acc: 91.18%, Val Acc: 76.25%\n",
    "Epoch 250/500, Loss: 20.7201, Train Acc: 90.82%, Val Acc: 76.46%\n",
    "Epoch 260/500, Loss: 20.9092, Train Acc: 91.24%, Val Acc: 76.46%\n",
    "Epoch 270/500, Loss: 20.2576, Train Acc: 91.55%, Val Acc: 75.62%\n",
    "Epoch 280/500, Loss: 20.3992, Train Acc: 91.91%, Val Acc: 76.67%\n",
    "Epoch 290/500, Loss: 20.1071, Train Acc: 92.44%, Val Acc: 77.50%\n",
    "🛑 Early Stopping at Epoch 298. Best Validation Accuracy: 78.54%\n",
    "\n",
    "✅ Client 2 Best Test Accuracy: 78.54%\n",
    "\n",
    "🔹 Training Model for Client 3...\n",
    "Epoch 10/500, Loss: 33.5534, Train Acc: 74.54%, Val Acc: 73.54%\n",
    "Epoch 20/500, Loss: 31.8265, Train Acc: 76.73%, Val Acc: 73.33%\n",
    "Epoch 30/500, Loss: 30.5611, Train Acc: 78.14%, Val Acc: 73.33%\n",
    "Epoch 40/500, Loss: 29.5722, Train Acc: 79.86%, Val Acc: 73.75%\n",
    "Epoch 50/500, Loss: 28.4772, Train Acc: 80.54%, Val Acc: 75.42%\n",
    "Epoch 60/500, Loss: 27.5022, Train Acc: 82.68%, Val Acc: 74.38%\n",
    "Epoch 70/500, Loss: 28.0754, Train Acc: 80.49%, Val Acc: 72.71%\n",
    "Epoch 80/500, Loss: 27.2415, Train Acc: 82.32%, Val Acc: 75.21%\n",
    "Epoch 90/500, Loss: 26.0268, Train Acc: 85.50%, Val Acc: 76.04%\n",
    "Epoch 100/500, Loss: 26.2210, Train Acc: 83.78%, Val Acc: 74.79%\n",
    "Epoch 110/500, Loss: 24.9350, Train Acc: 85.65%, Val Acc: 75.83%\n",
    "Epoch 120/500, Loss: 24.2676, Train Acc: 87.22%, Val Acc: 75.62%\n",
    "Epoch 130/500, Loss: 24.4917, Train Acc: 86.70%, Val Acc: 76.04%\n",
    "🛑 Early Stopping at Epoch 132. Best Validation Accuracy: 77.08%\n",
    "\n",
    "✅ Client 3 Best Test Accuracy: 77.08%\n",
    "\n",
    "🎯 Final Accuracies:\n",
    "Client 1: 78.75%\n",
    "Client 2: 78.54%\n",
    "Client 3: 77.08%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🔹 Using device: {device}\")\n",
    "\n",
    "# ✅ Updated Neural Network Model\n",
    "class OptimizedNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=768, num_classes=2, dropout_rate=0.1):\n",
    "        super(OptimizedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.drop1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.drop2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.drop2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# ✅ Load Data Function (No Balancing)\n",
    "def load_data(filename, batch_size=256):\n",
    "    df = pd.read_csv(filename)\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    print(f\"🔹 Class Distribution: {pd.Series(y).value_counts().to_dict()}\")\n",
    "\n",
    "    # ✅ Train/Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # ✅ Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # ✅ Convert to PyTorch Tensors\n",
    "    X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # ✅ Dataloaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = X_train.shape[1]\n",
    "    return train_loader, test_loader, input_size\n",
    "\n",
    "# ✅ Training Function\n",
    "def train_model(train_loader, test_loader, input_size, num_epochs=100, lr=2e-4, patience=10):\n",
    "    model = OptimizedNN(input_size).to(device)\n",
    "    \n",
    "    # ✅ Apply Label Smoothing to Loss Function\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=5e-5)  \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", factor=0.5, patience=5)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    scaler = torch.amp.GradScaler()  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, correct_train = 0.0, 0\n",
    "        total_train = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == y_batch).sum().item()\n",
    "            total_train += y_batch.size(0)\n",
    "\n",
    "        train_acc = correct_train / total_train * 100\n",
    "\n",
    "        # ✅ Validation Phase\n",
    "        model.eval()\n",
    "        correct_val, total_val = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == y_batch).sum().item()\n",
    "                total_val += y_batch.size(0)\n",
    "\n",
    "        val_acc = correct_val / total_val * 100\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {train_loss/total_train:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # ✅ Early Stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"🛑 Early Stopping at Epoch {epoch+1}. Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "            break\n",
    "\n",
    "    # ✅ Load Best Model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n✅ Best Test Accuracy: {best_val_acc:.2f}%\")\n",
    "    return best_val_acc\n",
    "\n",
    "# ✅ Load dataset & Train\n",
    "train_loader, test_loader, input_size = load_data(\"client_1.csv\", batch_size=256)\n",
    "train_model(train_loader, test_loader, input_size, num_epochs=100, lr=2e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ✅ Load dataset\n",
    "data_path = \"client_1.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# ✅ Assume the last column is the target variable\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# ✅ Feature Selection (Top 10 features)\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# ✅ Data Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ✅ Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ✅ Convert to Torch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ✅ Define MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout)]\n",
    "        layers.append(nn.Linear(hidden_dim, 2))  # Output layer for binary classification\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# ✅ Objective function for Optuna\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    learning_rate = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 256, step=32)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    # ✅ Data Loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ✅ Model\n",
    "    model = MLP(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout).to(\"cuda\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # ✅ Training Loop\n",
    "    best_acc = 0\n",
    "    for epoch in range(50):  # 50 Epochs for tuning\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ✅ Validation Accuracy\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X_test_tensor.to(\"cuda\"))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).cpu().numpy()\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            best_acc = max(best_acc, acc)\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "\n",
    "# ✅ Run Optuna for Hyperparameter Tuning\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)  # 20 Trials for tuning\n",
    "\n",
    "# ✅ Best Parameters\n",
    "best_params = study.best_params\n",
    "print(f\"✅ Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# ✅ Train Final Model with Best Hyperparameters\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "learning_rate = best_params[\"lr\"]\n",
    "hidden_dim = best_params[\"hidden_dim\"]\n",
    "num_layers = best_params[\"num_layers\"]\n",
    "dropout = best_params[\"dropout\"]\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout).to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ✅ Train Full Model\n",
    "best_acc = 0\n",
    "for epoch in range(100):  # Full Training\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # ✅ Validation Accuracy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor.to(\"cuda\"))\n",
    "        y_pred = torch.argmax(y_pred, dim=1).cpu().numpy()\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        best_acc = max(best_acc, acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/100 | Best Test Acc: {best_acc:.2%}\")\n",
    "\n",
    "print(f\"✅ Final Test Accuracy: {best_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"client_1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Encode categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"TenYearCHD\"] = label_encoder.fit_transform(df[\"TenYearCHD\"])\n",
    "\n",
    "# Feature Selection - Select Top 20 Best Features\n",
    "selector = SelectKBest(f_classif, k=20)\n",
    "X_selected = selector.fit_transform(df.drop(columns=[\"TenYearCHD\"]), df[\"TenYearCHD\"].values)\n",
    "y = df[\"TenYearCHD\"].values\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize Data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def create_dataloaders(batch_size):\n",
    "    train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = CustomDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Define Improved MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.BatchNorm1d(hidden_dim))  # Batch Normalization\n",
    "        layers.append(nn.GELU())  # Improved Activation Function\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, 2))  # Output layer for binary classification\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Hyperparameter Tuning with Optuna\n",
    "def objective(trial):\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 192, 256])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    train_loader, test_loader = create_dataloaders(batch_size)\n",
    "\n",
    "    model = MLP(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)  # Learning rate scheduler\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(15):  # Tune for 15 epochs per trial\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()  # Adjust learning rate\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        val_acc = correct / total\n",
    "        best_acc = max(best_acc, val_acc)\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "# Run Hyperparameter Optimization\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)  # Increase Trials for Better Results\n",
    "\n",
    "# Get Best Parameters\n",
    "best_params = study.best_params\n",
    "print(f\"✅ Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# Train Final Model with Best Params\n",
    "batch_size = best_params[\"batch_size\"]\n",
    "learning_rate = best_params[\"lr\"]\n",
    "hidden_dim = best_params[\"hidden_dim\"]\n",
    "num_layers = best_params[\"num_layers\"]\n",
    "dropout = best_params[\"dropout\"]\n",
    "\n",
    "train_loader, test_loader = create_dataloaders(batch_size)\n",
    "\n",
    "model = MLP(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)  # Longer decay\n",
    "\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(100):  # Train Final Model for 100 Epochs\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()  # Adjust Learning Rate\n",
    "\n",
    "    # Evaluate Model\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "    test_acc = correct / total\n",
    "    best_test_acc = max(best_test_acc, test_acc)\n",
    "    print(f\"Epoch {epoch+1}/100 | Best Test Acc: {best_test_acc:.2%}\")\n",
    "\n",
    "print(f\"✅ Final Test Accuracy: {best_test_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Best Test Acc: 69.79%\n",
      "Epoch 2/100 | Best Test Acc: 69.79%\n",
      "Epoch 3/100 | Best Test Acc: 69.79%\n",
      "Epoch 4/100 | Best Test Acc: 69.79%\n",
      "Epoch 5/100 | Best Test Acc: 69.79%\n",
      "Epoch 6/100 | Best Test Acc: 70.62%\n",
      "Epoch 7/100 | Best Test Acc: 71.46%\n",
      "Epoch 8/100 | Best Test Acc: 71.46%\n",
      "Epoch 9/100 | Best Test Acc: 71.88%\n",
      "Epoch 10/100 | Best Test Acc: 72.71%\n",
      "Epoch 11/100 | Best Test Acc: 72.92%\n",
      "Epoch 12/100 | Best Test Acc: 73.96%\n",
      "Epoch 13/100 | Best Test Acc: 73.96%\n",
      "Epoch 14/100 | Best Test Acc: 73.96%\n",
      "Epoch 15/100 | Best Test Acc: 73.96%\n",
      "Epoch 16/100 | Best Test Acc: 73.96%\n",
      "Epoch 17/100 | Best Test Acc: 73.96%\n",
      "Epoch 18/100 | Best Test Acc: 73.96%\n",
      "Epoch 19/100 | Best Test Acc: 73.96%\n",
      "Epoch 20/100 | Best Test Acc: 73.96%\n",
      "Epoch 21/100 | Best Test Acc: 73.96%\n",
      "Epoch 22/100 | Best Test Acc: 73.96%\n",
      "Epoch 23/100 | Best Test Acc: 73.96%\n",
      "Epoch 24/100 | Best Test Acc: 73.96%\n",
      "Epoch 25/100 | Best Test Acc: 73.96%\n",
      "Epoch 26/100 | Best Test Acc: 74.17%\n",
      "Epoch 27/100 | Best Test Acc: 74.17%\n",
      "Epoch 28/100 | Best Test Acc: 75.00%\n",
      "Epoch 29/100 | Best Test Acc: 75.00%\n",
      "Epoch 30/100 | Best Test Acc: 75.00%\n",
      "Epoch 31/100 | Best Test Acc: 75.00%\n",
      "Epoch 32/100 | Best Test Acc: 75.00%\n",
      "Epoch 33/100 | Best Test Acc: 75.00%\n",
      "Epoch 34/100 | Best Test Acc: 75.00%\n",
      "Epoch 35/100 | Best Test Acc: 75.00%\n",
      "Epoch 36/100 | Best Test Acc: 75.00%\n",
      "Epoch 37/100 | Best Test Acc: 75.00%\n",
      "Epoch 38/100 | Best Test Acc: 75.00%\n",
      "Epoch 39/100 | Best Test Acc: 75.00%\n",
      "Epoch 40/100 | Best Test Acc: 75.00%\n",
      "Epoch 41/100 | Best Test Acc: 75.00%\n",
      "Epoch 42/100 | Best Test Acc: 75.00%\n",
      "Epoch 43/100 | Best Test Acc: 75.00%\n",
      "Epoch 44/100 | Best Test Acc: 75.00%\n",
      "Epoch 45/100 | Best Test Acc: 75.00%\n",
      "Epoch 46/100 | Best Test Acc: 76.04%\n",
      "Epoch 47/100 | Best Test Acc: 76.04%\n",
      "Epoch 48/100 | Best Test Acc: 76.25%\n",
      "Epoch 49/100 | Best Test Acc: 76.25%\n",
      "Epoch 50/100 | Best Test Acc: 76.25%\n",
      "Epoch 51/100 | Best Test Acc: 76.25%\n",
      "Epoch 52/100 | Best Test Acc: 76.25%\n",
      "Epoch 53/100 | Best Test Acc: 76.25%\n",
      "Epoch 54/100 | Best Test Acc: 76.46%\n",
      "Epoch 55/100 | Best Test Acc: 76.46%\n",
      "Epoch 56/100 | Best Test Acc: 77.50%\n",
      "Epoch 57/100 | Best Test Acc: 77.50%\n",
      "Epoch 58/100 | Best Test Acc: 77.50%\n",
      "Epoch 59/100 | Best Test Acc: 77.50%\n",
      "Epoch 60/100 | Best Test Acc: 77.50%\n",
      "Epoch 61/100 | Best Test Acc: 77.50%\n",
      "Epoch 62/100 | Best Test Acc: 77.50%\n",
      "Epoch 63/100 | Best Test Acc: 77.50%\n",
      "Epoch 64/100 | Best Test Acc: 77.50%\n",
      "Epoch 65/100 | Best Test Acc: 77.50%\n",
      "Epoch 66/100 | Best Test Acc: 77.50%\n",
      "Epoch 67/100 | Best Test Acc: 77.50%\n",
      "Epoch 68/100 | Best Test Acc: 77.50%\n",
      "Epoch 69/100 | Best Test Acc: 77.50%\n",
      "Epoch 70/100 | Best Test Acc: 77.50%\n",
      "Epoch 71/100 | Best Test Acc: 77.50%\n",
      "Epoch 72/100 | Best Test Acc: 77.50%\n",
      "Epoch 73/100 | Best Test Acc: 77.50%\n",
      "Epoch 74/100 | Best Test Acc: 77.50%\n",
      "Epoch 75/100 | Best Test Acc: 77.50%\n",
      "Epoch 76/100 | Best Test Acc: 77.50%\n",
      "Epoch 77/100 | Best Test Acc: 77.50%\n",
      "Epoch 78/100 | Best Test Acc: 77.50%\n",
      "Epoch 79/100 | Best Test Acc: 77.50%\n",
      "Epoch 80/100 | Best Test Acc: 77.50%\n",
      "Epoch 81/100 | Best Test Acc: 77.50%\n",
      "Epoch 82/100 | Best Test Acc: 77.50%\n",
      "Epoch 83/100 | Best Test Acc: 77.50%\n",
      "Epoch 84/100 | Best Test Acc: 77.50%\n",
      "Epoch 85/100 | Best Test Acc: 77.50%\n",
      "Epoch 86/100 | Best Test Acc: 77.50%\n",
      "Epoch 87/100 | Best Test Acc: 77.50%\n",
      "Epoch 88/100 | Best Test Acc: 77.50%\n",
      "Epoch 89/100 | Best Test Acc: 77.50%\n",
      "Epoch 90/100 | Best Test Acc: 77.50%\n",
      "Epoch 91/100 | Best Test Acc: 77.50%\n",
      "Epoch 92/100 | Best Test Acc: 77.50%\n",
      "Epoch 93/100 | Best Test Acc: 77.50%\n",
      "Epoch 94/100 | Best Test Acc: 77.50%\n",
      "Epoch 95/100 | Best Test Acc: 77.50%\n",
      "Epoch 96/100 | Best Test Acc: 77.50%\n",
      "Epoch 97/100 | Best Test Acc: 77.50%\n",
      "Epoch 98/100 | Best Test Acc: 77.50%\n",
      "Epoch 99/100 | Best Test Acc: 77.50%\n",
      "Epoch 100/100 | Best Test Acc: 77.50%\n",
      "✅ Final Test Accuracy: 77.50%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# ✅ Load dataset\n",
    "file_path = \"client_1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ✅ Ensure target column exists\n",
    "target_column = \"TenYearCHD\"\n",
    "if target_column not in df.columns:\n",
    "    raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
    "\n",
    "# ✅ Separate Features and Target\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column].values\n",
    "\n",
    "# ✅ Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# ✅ Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# ✅ Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# ✅ Create Custom Dataset & DataLoaders\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def create_dataloaders(batch_size=32):\n",
    "    train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = CustomDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ✅ Define Optimized MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=192, num_layers=2, dropout=0.1155):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.BatchNorm1d(hidden_dim))  # Batch Normalization\n",
    "        layers.append(nn.GELU())  # Improved Activation Function\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, 2))  # Output Layer for Binary Classification\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ✅ Train Model using the Best Hyperparameters\n",
    "def train_and_evaluate(batch_size=32, learning_rate=0.0031, hidden_dim=192, num_layers=2, dropout=0.1155, num_epochs=100):\n",
    "    train_loader, test_loader = create_dataloaders(batch_size)\n",
    "\n",
    "    model = MLP(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)  # Adaptive Learning Rate\n",
    "\n",
    "    best_test_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()  # Adjust Learning Rate\n",
    "\n",
    "        # ✅ Evaluate Model\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        test_acc = correct / total\n",
    "        best_test_acc = max(best_test_acc, test_acc)\n",
    "        print(f\"Epoch {epoch+1}/100 | Best Test Acc: {best_test_acc:.2%}\")\n",
    "\n",
    "    print(f\"✅ Final Test Accuracy: {best_test_acc:.2%}\")\n",
    "\n",
    "# ✅ Run Training\n",
    "train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Best Test Acc: 67.08%\n",
      "Epoch 2/100 | Best Test Acc: 71.67%\n",
      "Epoch 3/100 | Best Test Acc: 71.67%\n",
      "Epoch 4/100 | Best Test Acc: 71.88%\n",
      "Epoch 5/100 | Best Test Acc: 71.88%\n",
      "Epoch 6/100 | Best Test Acc: 72.71%\n",
      "Epoch 7/100 | Best Test Acc: 74.58%\n",
      "Epoch 8/100 | Best Test Acc: 74.58%\n",
      "Epoch 9/100 | Best Test Acc: 74.58%\n",
      "Epoch 10/100 | Best Test Acc: 74.58%\n",
      "Epoch 11/100 | Best Test Acc: 74.58%\n",
      "Epoch 12/100 | Best Test Acc: 74.58%\n",
      "Epoch 13/100 | Best Test Acc: 74.58%\n",
      "Epoch 14/100 | Best Test Acc: 75.21%\n",
      "Epoch 15/100 | Best Test Acc: 75.42%\n",
      "Epoch 16/100 | Best Test Acc: 75.42%\n",
      "Epoch 17/100 | Best Test Acc: 75.42%\n",
      "Epoch 18/100 | Best Test Acc: 75.83%\n",
      "Epoch 19/100 | Best Test Acc: 75.83%\n",
      "Epoch 20/100 | Best Test Acc: 75.83%\n",
      "Epoch 21/100 | Best Test Acc: 75.83%\n",
      "Epoch 22/100 | Best Test Acc: 75.83%\n",
      "Epoch 23/100 | Best Test Acc: 75.83%\n",
      "Epoch 24/100 | Best Test Acc: 75.83%\n",
      "Epoch 25/100 | Best Test Acc: 75.83%\n",
      "Epoch 26/100 | Best Test Acc: 75.83%\n",
      "Epoch 27/100 | Best Test Acc: 75.83%\n",
      "Epoch 28/100 | Best Test Acc: 75.83%\n",
      "Epoch 29/100 | Best Test Acc: 75.83%\n",
      "Epoch 30/100 | Best Test Acc: 75.83%\n",
      "Epoch 31/100 | Best Test Acc: 75.83%\n",
      "Epoch 32/100 | Best Test Acc: 75.83%\n",
      "Epoch 33/100 | Best Test Acc: 75.83%\n",
      "Epoch 34/100 | Best Test Acc: 75.83%\n",
      "Epoch 35/100 | Best Test Acc: 75.83%\n",
      "Epoch 36/100 | Best Test Acc: 76.25%\n",
      "Epoch 37/100 | Best Test Acc: 76.25%\n",
      "Epoch 38/100 | Best Test Acc: 76.25%\n",
      "Epoch 39/100 | Best Test Acc: 76.25%\n",
      "Epoch 40/100 | Best Test Acc: 76.25%\n",
      "Epoch 41/100 | Best Test Acc: 76.25%\n",
      "Epoch 42/100 | Best Test Acc: 76.25%\n",
      "Epoch 43/100 | Best Test Acc: 76.25%\n",
      "Epoch 44/100 | Best Test Acc: 77.50%\n",
      "Epoch 45/100 | Best Test Acc: 77.50%\n",
      "Epoch 46/100 | Best Test Acc: 77.50%\n",
      "Epoch 47/100 | Best Test Acc: 77.50%\n",
      "Epoch 48/100 | Best Test Acc: 77.50%\n",
      "Epoch 49/100 | Best Test Acc: 77.50%\n",
      "Epoch 50/100 | Best Test Acc: 77.50%\n",
      "Epoch 51/100 | Best Test Acc: 77.50%\n",
      "Epoch 52/100 | Best Test Acc: 77.50%\n",
      "Epoch 53/100 | Best Test Acc: 77.50%\n",
      "Epoch 54/100 | Best Test Acc: 77.50%\n",
      "Epoch 55/100 | Best Test Acc: 77.50%\n",
      "Epoch 56/100 | Best Test Acc: 77.50%\n",
      "Epoch 57/100 | Best Test Acc: 77.50%\n",
      "Epoch 58/100 | Best Test Acc: 77.50%\n",
      "Epoch 59/100 | Best Test Acc: 77.71%\n",
      "Epoch 60/100 | Best Test Acc: 77.71%\n",
      "Epoch 61/100 | Best Test Acc: 77.71%\n",
      "Epoch 62/100 | Best Test Acc: 77.71%\n",
      "Epoch 63/100 | Best Test Acc: 77.71%\n",
      "Epoch 64/100 | Best Test Acc: 77.71%\n",
      "Epoch 65/100 | Best Test Acc: 77.71%\n",
      "Epoch 66/100 | Best Test Acc: 77.71%\n",
      "Epoch 67/100 | Best Test Acc: 77.71%\n",
      "Epoch 68/100 | Best Test Acc: 77.71%\n",
      "Epoch 69/100 | Best Test Acc: 77.71%\n",
      "Epoch 70/100 | Best Test Acc: 77.71%\n",
      "Epoch 71/100 | Best Test Acc: 77.71%\n",
      "Epoch 72/100 | Best Test Acc: 77.71%\n",
      "Epoch 73/100 | Best Test Acc: 78.75%\n",
      "Epoch 74/100 | Best Test Acc: 79.17%\n",
      "Epoch 75/100 | Best Test Acc: 79.17%\n",
      "Epoch 76/100 | Best Test Acc: 79.17%\n",
      "Epoch 77/100 | Best Test Acc: 79.17%\n",
      "Epoch 78/100 | Best Test Acc: 79.17%\n",
      "Epoch 79/100 | Best Test Acc: 79.17%\n",
      "Epoch 80/100 | Best Test Acc: 79.17%\n",
      "Epoch 81/100 | Best Test Acc: 79.17%\n",
      "Epoch 82/100 | Best Test Acc: 79.17%\n",
      "Epoch 83/100 | Best Test Acc: 79.17%\n",
      "Epoch 84/100 | Best Test Acc: 79.17%\n",
      "Epoch 85/100 | Best Test Acc: 79.58%\n",
      "Epoch 86/100 | Best Test Acc: 79.58%\n",
      "Epoch 87/100 | Best Test Acc: 79.58%\n",
      "Epoch 88/100 | Best Test Acc: 79.58%\n",
      "Epoch 89/100 | Best Test Acc: 79.58%\n",
      "Epoch 90/100 | Best Test Acc: 79.58%\n",
      "Epoch 91/100 | Best Test Acc: 79.58%\n",
      "Epoch 92/100 | Best Test Acc: 79.58%\n",
      "Epoch 93/100 | Best Test Acc: 79.58%\n",
      "Epoch 94/100 | Best Test Acc: 79.58%\n",
      "Epoch 95/100 | Best Test Acc: 79.58%\n",
      "Epoch 96/100 | Best Test Acc: 79.58%\n",
      "Epoch 97/100 | Best Test Acc: 79.58%\n",
      "Epoch 98/100 | Best Test Acc: 79.58%\n",
      "Epoch 99/100 | Best Test Acc: 79.58%\n",
      "Epoch 100/100 | Best Test Acc: 79.58%\n",
      "✅ Final Test Accuracy: 79.58%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ✅ Load dataset\n",
    "file_path = \"client_1.csv\"  # Adjust path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ✅ Ensure target column exists\n",
    "target_column = \"TenYearCHD\"\n",
    "if target_column not in df.columns:\n",
    "    raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
    "\n",
    "# ✅ Separate Features and Target\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column].values\n",
    "\n",
    "# ✅ Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# ✅ Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# ✅ Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# ✅ Create Custom Dataset & DataLoaders\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def create_dataloaders(batch_size=64):\n",
    "    train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = CustomDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ✅ Define Optimized MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=4, dropout=0.2587):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.BatchNorm1d(hidden_dim))  # Batch Normalization\n",
    "        layers.append(nn.GELU())  # Improved Activation Function\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, 2))  # Output Layer for Binary Classification\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ✅ Train Model using the Best Hyperparameters\n",
    "def train_and_evaluate(batch_size=64, learning_rate=0.00507, hidden_dim=128, num_layers=4, dropout=0.2587, num_epochs=100):\n",
    "    train_loader, test_loader = create_dataloaders(batch_size)\n",
    "\n",
    "    model = MLP(input_dim=X_train.shape[1], hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)  # Adaptive Learning Rate\n",
    "\n",
    "    best_test_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()  # Adjust Learning Rate\n",
    "\n",
    "        # ✅ Evaluate Model\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        test_acc = correct / total\n",
    "        best_test_acc = max(best_test_acc, test_acc)\n",
    "        print(f\"Epoch {epoch+1}/100 | Best Test Acc: {best_test_acc:.2%}\")\n",
    "\n",
    "    print(f\"✅ Final Test Accuracy: {best_test_acc:.2%}\")\n",
    "\n",
    "# ✅ Run Training\n",
    "train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU only'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n"
     ]
    },
    {
     "ename": "_MultiThreadedRendezvous",
     "evalue": "<_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:8081: ConnectEx: Connection refused (No connection could be made because the target machine actively refused it.\r\n -- 10061)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:8081: ConnectEx: Connection refused (No connection could be made because the target machine actively refused it.\\r\\n -- 10061)\", grpc_status:14, created_time:\"2025-03-17T20:25:53.3790492+00:00\"}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 159\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(avg_loss), \u001b[38;5;28mlen\u001b[39m(test_loader\u001b[38;5;241m.\u001b[39mdataset), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy}\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# ✅ Start the Federated Learning Client\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m \u001b[43mfl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m127.0.0.1:8081\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use .to_client() to avoid deprecation warning\u001b[39;49;00m\n\u001b[0;32m    162\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Rreserch work\\fedenvioremnt\\.venv\\lib\\site-packages\\flwr\\client\\app.py:201\u001b[0m, in \u001b[0;36mstart_client\u001b[1;34m(server_address, client_fn, client, grpc_max_message_length, root_certificates, insecure, transport, authentication_keys, max_retries, max_wait_time)\u001b[0m\n\u001b[0;32m    198\u001b[0m warn_deprecated_feature(name\u001b[38;5;241m=\u001b[39mmsg)\n\u001b[0;32m    200\u001b[0m event(EventType\u001b[38;5;241m.\u001b[39mSTART_CLIENT_ENTER)\n\u001b[1;32m--> 201\u001b[0m \u001b[43mstart_client_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserver_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_client_app_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrpc_max_message_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrpc_max_message_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_certificates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_certificates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43minsecure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minsecure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauthentication_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauthentication_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m event(EventType\u001b[38;5;241m.\u001b[39mSTART_CLIENT_LEAVE)\n",
      "File \u001b[1;32md:\\Rreserch work\\fedenvioremnt\\.venv\\lib\\site-packages\\flwr\\client\\app.py:438\u001b[0m, in \u001b[0;36mstart_client_internal\u001b[1;34m(server_address, node_config, load_client_app_fn, client_fn, client, grpc_max_message_length, root_certificates, insecure, transport, authentication_keys, max_retries, max_wait_time, flwr_path, isolation, clientappio_api_address)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;66;03m# Receive\u001b[39;00m\n\u001b[1;32m--> 438\u001b[0m         message \u001b[38;5;241m=\u001b[39m \u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Wait for 3s before asking again\u001b[39;00m\n",
      "File \u001b[1;32md:\\Rreserch work\\fedenvioremnt\\.venv\\lib\\site-packages\\flwr\\client\\grpc_client\\connection.py:140\u001b[0m, in \u001b[0;36mgrpc_connection.<locals>.receive\u001b[1;34m()\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreceive\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# Receive ServerMessage proto\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mserver_message_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# ServerMessage proto --> *Ins --> RecordSet\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     field \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mWhichOneof(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Rreserch work\\fedenvioremnt\\.venv\\lib\\site-packages\\grpc\\_channel.py:543\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Rreserch work\\fedenvioremnt\\.venv\\lib\\site-packages\\grpc\\_channel.py:969\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m()\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:8081: ConnectEx: Connection refused (No connection could be made because the target machine actively refused it.\r\n -- 10061)\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:8081: ConnectEx: Connection refused (No connection could be made because the target machine actively refused it.\\r\\n -- 10061)\", grpc_status:14, created_time:\"2025-03-17T20:25:53.3790492+00:00\"}\"\n>"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import flwr as fl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# ✅ Set device for computation (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ Load dataset\n",
    "file_path = \"client_2.csv\"  # Adjust path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ✅ Ensure target column exists\n",
    "target_column = \"TenYearCHD\"\n",
    "if target_column not in df.columns:\n",
    "    raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
    "\n",
    "# ✅ Separate Features and Target\n",
    "X = df.drop(columns=[target_column]).values\n",
    "y = df[target_column].values\n",
    "\n",
    "# ✅ Standardize Features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# ✅ Train/Test Split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ✅ Convert to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# ✅ Create Custom Dataset & DataLoaders\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def create_dataloaders(batch_size=64):\n",
    "    train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = CustomDataset(X_test_tensor, y_test_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = create_dataloaders()\n",
    "\n",
    "# ✅ Define MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=4, dropout=0.2587):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = [\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.GELU()\n",
    "        ]\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "        layers.append(nn.Linear(hidden_dim, 2))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ✅ Initialize model\n",
    "input_dim = X_train.shape[1]\n",
    "model = MLP(input_dim=input_dim).to(device)\n",
    "\n",
    "# ✅ Define optimizer, loss function, and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00507)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25)\n",
    "\n",
    "# ✅ Fix Parameter Handling\n",
    "def get_parameters(model: nn.Module) -> List[np.ndarray]:\n",
    "    return [val.cpu().detach().numpy() for val in model.state_dict().values()]\n",
    "\n",
    "def set_parameters(model: nn.Module, parameters: List[np.ndarray]) -> None:\n",
    "    state_dict = model.state_dict()\n",
    "    new_state_dict = {k: torch.tensor(v).to(device) for k, v in zip(state_dict.keys(), parameters)}\n",
    "    model.load_state_dict(new_state_dict, strict=True)\n",
    "\n",
    "# ✅ Define the Federated Learning Client\n",
    "class FLClient(fl.client.NumPyClient):\n",
    "    def get_parameters(self, config: Dict[str, str]) -> List[np.ndarray]:\n",
    "        return get_parameters(model)\n",
    "\n",
    "    def set_parameters(self, parameters: List[np.ndarray]) -> None:\n",
    "        set_parameters(model, parameters)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Train the local model and return updated parameters.\"\"\"\n",
    "        print(\"📤 Client received parameters for training.\")\n",
    "        self.set_parameters(parameters)\n",
    "        model.train()\n",
    "        \n",
    "        local_epochs = 50  # Ensure sufficient local training\n",
    "        total_loss = 0.0\n",
    "        for epoch in range(local_epochs):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"✅ Client training complete. Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # ✅ **Return three values to avoid ValueError**\n",
    "        return self.get_parameters(config), len(train_loader.dataset), {\"loss\": avg_loss}\n",
    "\n",
    "    def evaluate(self, parameters: List[np.ndarray], config: Dict[str, str]) -> Tuple[float, int, Dict]:\n",
    "        \"\"\"Evaluate the model.\"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        model.eval()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "                total += y_batch.size(0)\n",
    "\n",
    "        accuracy = correct / total if total > 0 else 0.0\n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        print(f\"✅ Evaluation: Loss={avg_loss:.4f}, Accuracy={accuracy:.4f}\")\n",
    "\n",
    "        # ✅ **Return three values (loss, dataset size, accuracy)**\n",
    "        return float(avg_loss), len(test_loader.dataset), {\"accuracy\": accuracy}\n",
    "\n",
    "# ✅ Start the Federated Learning Client\n",
    "fl.client.start_client(\n",
    "    server_address=\"127.0.0.1:8081\",\n",
    "    client=FLClient().to_client(),  # Use .to_client() to avoid deprecation warning\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing \n",
    "client and server (my text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.client.start_client() is deprecated.\n",
      "\tInstead, use the `flower-supernode` CLI command to start a SuperNode as shown below:\n",
      "\n",
      "\t\t$ flower-supernode --insecure --superlink='<IP>:<PORT>'\n",
      "\n",
      "\tTo view all available options, run:\n",
      "\n",
      "\t\t$ flower-supernode --help\n",
      "\n",
      "\tUsing `start_client()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message afa2fb4a-981f-469a-bd0c-e6b5379dba56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Client 1, round 1] Training for 60 epochs...\n",
      "Epoch 1/60, Loss: 0.6858\n",
      "Epoch 2/60, Loss: 0.6588\n",
      "Epoch 3/60, Loss: 0.6374\n",
      "Epoch 4/60, Loss: 0.6217\n",
      "Epoch 5/60, Loss: 0.6101\n",
      "Epoch 6/60, Loss: 0.6004\n",
      "Epoch 7/60, Loss: 0.5943\n",
      "Epoch 8/60, Loss: 0.5869\n",
      "Epoch 9/60, Loss: 0.5805\n",
      "Epoch 10/60, Loss: 0.5756\n",
      "Epoch 11/60, Loss: 0.5714\n",
      "Epoch 12/60, Loss: 0.5677\n",
      "Epoch 13/60, Loss: 0.5634\n",
      "Epoch 14/60, Loss: 0.5582\n",
      "Epoch 15/60, Loss: 0.5550\n",
      "Epoch 16/60, Loss: 0.5502\n",
      "Epoch 17/60, Loss: 0.5466\n",
      "Epoch 18/60, Loss: 0.5438\n",
      "Epoch 19/60, Loss: 0.5381\n",
      "Epoch 20/60, Loss: 0.5355\n",
      "Epoch 21/60, Loss: 0.5313\n",
      "Epoch 22/60, Loss: 0.5279\n",
      "Epoch 23/60, Loss: 0.5251\n",
      "Epoch 24/60, Loss: 0.5191\n",
      "Epoch 25/60, Loss: 0.5172\n",
      "Epoch 26/60, Loss: 0.5145\n",
      "Epoch 27/60, Loss: 0.5088\n",
      "Epoch 28/60, Loss: 0.5076\n",
      "Epoch 29/60, Loss: 0.5031\n",
      "Epoch 30/60, Loss: 0.4991\n",
      "Epoch 31/60, Loss: 0.4947\n",
      "Epoch 32/60, Loss: 0.4948\n",
      "Epoch 33/60, Loss: 0.4946\n",
      "Epoch 34/60, Loss: 0.4854\n",
      "Epoch 35/60, Loss: 0.4838\n",
      "Epoch 36/60, Loss: 0.4795\n",
      "Epoch 37/60, Loss: 0.4766\n",
      "Epoch 38/60, Loss: 0.4753\n",
      "Epoch 39/60, Loss: 0.4703\n",
      "Epoch 40/60, Loss: 0.4680\n",
      "Epoch 41/60, Loss: 0.4658\n",
      "Epoch 42/60, Loss: 0.4617\n",
      "Epoch 43/60, Loss: 0.4599\n",
      "Epoch 44/60, Loss: 0.4563\n",
      "Epoch 45/60, Loss: 0.4551\n",
      "Epoch 46/60, Loss: 0.4537\n",
      "Epoch 47/60, Loss: 0.4500\n",
      "Epoch 48/60, Loss: 0.4464\n",
      "Epoch 49/60, Loss: 0.4436\n",
      "Epoch 50/60, Loss: 0.4400\n",
      "Epoch 51/60, Loss: 0.4374\n",
      "Epoch 52/60, Loss: 0.4371\n",
      "Epoch 53/60, Loss: 0.4323\n",
      "Epoch 54/60, Loss: 0.4294\n",
      "Epoch 55/60, Loss: 0.4294\n",
      "Epoch 56/60, Loss: 0.4256\n",
      "Epoch 57/60, Loss: 0.4223\n",
      "Epoch 58/60, Loss: 0.4194\n",
      "Epoch 59/60, Loss: 0.4188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message 058071b4-aa43-45f8-be5f-9736e3385d5d\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 7ff15dc3-cbe9-4b82-a1af-13c5c4c34181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60, Loss: 0.4148\n",
      "[Client 1] Sending model parameters to server.\n",
      "[Client 1] Evaluating model...\n",
      "Validation Loss: 0.7239, Accuracy: 0.7042\n",
      "[Client 1, round 1] Training for 60 epochs...\n",
      "Epoch 1/60, Loss: 0.4986\n",
      "Epoch 2/60, Loss: 0.4834\n",
      "Epoch 3/60, Loss: 0.4779\n",
      "Epoch 4/60, Loss: 0.4704\n",
      "Epoch 5/60, Loss: 0.4671\n",
      "Epoch 6/60, Loss: 0.4634\n",
      "Epoch 7/60, Loss: 0.4563\n",
      "Epoch 8/60, Loss: 0.4537\n",
      "Epoch 9/60, Loss: 0.4488\n",
      "Epoch 10/60, Loss: 0.4462\n",
      "Epoch 11/60, Loss: 0.4427\n",
      "Epoch 12/60, Loss: 0.4383\n",
      "Epoch 13/60, Loss: 0.4344\n",
      "Epoch 14/60, Loss: 0.4332\n",
      "Epoch 15/60, Loss: 0.4302\n",
      "Epoch 16/60, Loss: 0.4281\n",
      "Epoch 17/60, Loss: 0.4243\n",
      "Epoch 18/60, Loss: 0.4205\n",
      "Epoch 19/60, Loss: 0.4172\n",
      "Epoch 20/60, Loss: 0.4143\n",
      "Epoch 21/60, Loss: 0.4130\n",
      "Epoch 22/60, Loss: 0.4105\n",
      "Epoch 23/60, Loss: 0.4067\n",
      "Epoch 24/60, Loss: 0.4044\n",
      "Epoch 25/60, Loss: 0.3998\n",
      "Epoch 26/60, Loss: 0.3990\n",
      "Epoch 27/60, Loss: 0.3944\n",
      "Epoch 28/60, Loss: 0.3928\n",
      "Epoch 29/60, Loss: 0.3897\n",
      "Epoch 30/60, Loss: 0.3845\n",
      "Epoch 31/60, Loss: 0.3872\n",
      "Epoch 32/60, Loss: 0.3836\n",
      "Epoch 33/60, Loss: 0.3790\n",
      "Epoch 34/60, Loss: 0.3770\n",
      "Epoch 35/60, Loss: 0.3732\n",
      "Epoch 36/60, Loss: 0.3729\n",
      "Epoch 37/60, Loss: 0.3690\n",
      "Epoch 38/60, Loss: 0.3666\n",
      "Epoch 39/60, Loss: 0.3630\n",
      "Epoch 40/60, Loss: 0.3638\n",
      "Epoch 41/60, Loss: 0.3606\n",
      "Epoch 42/60, Loss: 0.3565\n",
      "Epoch 43/60, Loss: 0.3541\n",
      "Epoch 44/60, Loss: 0.3539\n",
      "Epoch 45/60, Loss: 0.3523\n",
      "Epoch 46/60, Loss: 0.3489\n",
      "Epoch 47/60, Loss: 0.3490\n",
      "Epoch 48/60, Loss: 0.3451\n",
      "Epoch 49/60, Loss: 0.3423\n",
      "Epoch 50/60, Loss: 0.3395\n",
      "Epoch 51/60, Loss: 0.3387\n",
      "Epoch 52/60, Loss: 0.3352\n",
      "Epoch 53/60, Loss: 0.3337\n",
      "Epoch 54/60, Loss: 0.3313\n",
      "Epoch 55/60, Loss: 0.3298\n",
      "Epoch 56/60, Loss: 0.3255\n",
      "Epoch 57/60, Loss: 0.3261\n",
      "Epoch 58/60, Loss: 0.3214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message 5d1db0ac-7b2d-4958-946a-e033d76d0ca2\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 5959ca36-1350-43af-8b34-cd6ea9427d64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/60, Loss: 0.3220\n",
      "Epoch 60/60, Loss: 0.3190\n",
      "[Client 1] Sending model parameters to server.\n",
      "[Client 1] Evaluating model...\n",
      "Validation Loss: 0.7985, Accuracy: 0.7292\n",
      "[Client 1, round 1] Training for 60 epochs...\n",
      "Epoch 1/60, Loss: 0.4077\n",
      "Epoch 2/60, Loss: 0.3915\n",
      "Epoch 3/60, Loss: 0.3866\n",
      "Epoch 4/60, Loss: 0.3797\n",
      "Epoch 5/60, Loss: 0.3720\n",
      "Epoch 6/60, Loss: 0.3668\n",
      "Epoch 7/60, Loss: 0.3617\n",
      "Epoch 8/60, Loss: 0.3585\n",
      "Epoch 9/60, Loss: 0.3563\n",
      "Epoch 10/60, Loss: 0.3514\n",
      "Epoch 11/60, Loss: 0.3483\n",
      "Epoch 12/60, Loss: 0.3428\n",
      "Epoch 13/60, Loss: 0.3422\n",
      "Epoch 14/60, Loss: 0.3361\n",
      "Epoch 15/60, Loss: 0.3361\n",
      "Epoch 16/60, Loss: 0.3322\n",
      "Epoch 17/60, Loss: 0.3309\n",
      "Epoch 18/60, Loss: 0.3268\n",
      "Epoch 19/60, Loss: 0.3244\n",
      "Epoch 20/60, Loss: 0.3228\n",
      "Epoch 21/60, Loss: 0.3207\n",
      "Epoch 22/60, Loss: 0.3219\n",
      "Epoch 23/60, Loss: 0.3145\n",
      "Epoch 24/60, Loss: 0.3134\n",
      "Epoch 25/60, Loss: 0.3109\n",
      "Epoch 26/60, Loss: 0.3095\n",
      "Epoch 27/60, Loss: 0.3067\n",
      "Epoch 28/60, Loss: 0.3027\n",
      "Epoch 29/60, Loss: 0.3047\n",
      "Epoch 30/60, Loss: 0.2977\n",
      "Epoch 31/60, Loss: 0.2962\n",
      "Epoch 32/60, Loss: 0.2979\n",
      "Epoch 33/60, Loss: 0.2957\n",
      "Epoch 34/60, Loss: 0.2912\n",
      "Epoch 35/60, Loss: 0.2890\n",
      "Epoch 36/60, Loss: 0.2893\n",
      "Epoch 37/60, Loss: 0.2879\n",
      "Epoch 38/60, Loss: 0.2878\n",
      "Epoch 39/60, Loss: 0.2859\n",
      "Epoch 40/60, Loss: 0.2819\n",
      "Epoch 41/60, Loss: 0.2784\n",
      "Epoch 42/60, Loss: 0.2784\n",
      "Epoch 43/60, Loss: 0.2774\n",
      "Epoch 44/60, Loss: 0.2747\n",
      "Epoch 45/60, Loss: 0.2711\n",
      "Epoch 46/60, Loss: 0.2713\n",
      "Epoch 47/60, Loss: 0.2695\n",
      "Epoch 48/60, Loss: 0.2659\n",
      "Epoch 49/60, Loss: 0.2635\n",
      "Epoch 50/60, Loss: 0.2670\n",
      "Epoch 51/60, Loss: 0.2613\n",
      "Epoch 52/60, Loss: 0.2593\n",
      "Epoch 53/60, Loss: 0.2622\n",
      "Epoch 54/60, Loss: 0.2552\n",
      "Epoch 55/60, Loss: 0.2580\n",
      "Epoch 56/60, Loss: 0.2562\n",
      "Epoch 57/60, Loss: 0.2538\n",
      "Epoch 58/60, Loss: 0.2506\n",
      "Epoch 59/60, Loss: 0.2498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message c78291f2-f690-48a1-9e73-44f27a94b264\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message 0eff7644-a512-40d8-88d5-65f04900c349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60, Loss: 0.2493\n",
      "[Client 1] Sending model parameters to server.\n",
      "[Client 1] Evaluating model...\n",
      "Validation Loss: 0.9101, Accuracy: 0.7292\n",
      "[Client 1, round 1] Training for 60 epochs...\n",
      "Epoch 1/60, Loss: 0.3480\n",
      "Epoch 2/60, Loss: 0.3298\n",
      "Epoch 3/60, Loss: 0.3212\n",
      "Epoch 4/60, Loss: 0.3133\n",
      "Epoch 5/60, Loss: 0.3054\n",
      "Epoch 6/60, Loss: 0.3033\n",
      "Epoch 7/60, Loss: 0.2952\n",
      "Epoch 8/60, Loss: 0.2955\n",
      "Epoch 9/60, Loss: 0.2873\n",
      "Epoch 10/60, Loss: 0.2805\n",
      "Epoch 11/60, Loss: 0.2805\n",
      "Epoch 12/60, Loss: 0.2764\n",
      "Epoch 13/60, Loss: 0.2704\n",
      "Epoch 14/60, Loss: 0.2720\n",
      "Epoch 15/60, Loss: 0.2671\n",
      "Epoch 16/60, Loss: 0.2661\n",
      "Epoch 17/60, Loss: 0.2677\n",
      "Epoch 18/60, Loss: 0.2617\n",
      "Epoch 19/60, Loss: 0.2591\n",
      "Epoch 20/60, Loss: 0.2564\n",
      "Epoch 21/60, Loss: 0.2526\n",
      "Epoch 22/60, Loss: 0.2524\n",
      "Epoch 23/60, Loss: 0.2497\n",
      "Epoch 24/60, Loss: 0.2493\n",
      "Epoch 25/60, Loss: 0.2468\n",
      "Epoch 26/60, Loss: 0.2456\n",
      "Epoch 27/60, Loss: 0.2405\n",
      "Epoch 28/60, Loss: 0.2422\n",
      "Epoch 29/60, Loss: 0.2376\n",
      "Epoch 30/60, Loss: 0.2376\n",
      "Epoch 31/60, Loss: 0.2365\n",
      "Epoch 32/60, Loss: 0.2373\n",
      "Epoch 33/60, Loss: 0.2326\n",
      "Epoch 34/60, Loss: 0.2353\n",
      "Epoch 35/60, Loss: 0.2309\n",
      "Epoch 36/60, Loss: 0.2288\n",
      "Epoch 37/60, Loss: 0.2313\n",
      "Epoch 38/60, Loss: 0.2247\n",
      "Epoch 39/60, Loss: 0.2235\n",
      "Epoch 40/60, Loss: 0.2231\n",
      "Epoch 41/60, Loss: 0.2207\n",
      "Epoch 42/60, Loss: 0.2206\n",
      "Epoch 43/60, Loss: 0.2205\n",
      "Epoch 44/60, Loss: 0.2161\n",
      "Epoch 45/60, Loss: 0.2144\n",
      "Epoch 46/60, Loss: 0.2143\n",
      "Epoch 47/60, Loss: 0.2125\n",
      "Epoch 48/60, Loss: 0.2112\n",
      "Epoch 49/60, Loss: 0.2086\n",
      "Epoch 50/60, Loss: 0.2116\n",
      "Epoch 51/60, Loss: 0.2037\n",
      "Epoch 52/60, Loss: 0.2053\n",
      "Epoch 53/60, Loss: 0.2034\n",
      "Epoch 54/60, Loss: 0.2022\n",
      "Epoch 55/60, Loss: 0.2013\n",
      "Epoch 56/60, Loss: 0.1991\n",
      "Epoch 57/60, Loss: 0.2004\n",
      "Epoch 58/60, Loss: 0.1951\n",
      "Epoch 59/60, Loss: 0.1986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message 14232d5b-7b60-411e-a80f-65b1571f6c7b\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: train message ef40f4c5-7747-49c9-8203-9bbc654d0eaf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60, Loss: 0.1984\n",
      "[Client 1] Sending model parameters to server.\n",
      "[Client 1] Evaluating model...\n",
      "Validation Loss: 1.0748, Accuracy: 0.7812\n",
      "[Client 1, round 1] Training for 60 epochs...\n",
      "Epoch 1/60, Loss: 0.3095\n",
      "Epoch 2/60, Loss: 0.2872\n",
      "Epoch 3/60, Loss: 0.2738\n",
      "Epoch 4/60, Loss: 0.2650\n",
      "Epoch 5/60, Loss: 0.2576\n",
      "Epoch 6/60, Loss: 0.2507\n",
      "Epoch 7/60, Loss: 0.2448\n",
      "Epoch 8/60, Loss: 0.2396\n",
      "Epoch 9/60, Loss: 0.2385\n",
      "Epoch 10/60, Loss: 0.2323\n",
      "Epoch 11/60, Loss: 0.2346\n",
      "Epoch 12/60, Loss: 0.2253\n",
      "Epoch 13/60, Loss: 0.2219\n",
      "Epoch 14/60, Loss: 0.2206\n",
      "Epoch 15/60, Loss: 0.2198\n",
      "Epoch 16/60, Loss: 0.2170\n",
      "Epoch 17/60, Loss: 0.2120\n",
      "Epoch 18/60, Loss: 0.2106\n",
      "Epoch 19/60, Loss: 0.2118\n",
      "Epoch 20/60, Loss: 0.2059\n",
      "Epoch 21/60, Loss: 0.2042\n",
      "Epoch 22/60, Loss: 0.2002\n",
      "Epoch 23/60, Loss: 0.1989\n",
      "Epoch 24/60, Loss: 0.2030\n",
      "Epoch 25/60, Loss: 0.1987\n",
      "Epoch 26/60, Loss: 0.1956\n",
      "Epoch 27/60, Loss: 0.1977\n",
      "Epoch 28/60, Loss: 0.1950\n",
      "Epoch 29/60, Loss: 0.1926\n",
      "Epoch 30/60, Loss: 0.1899\n",
      "Epoch 31/60, Loss: 0.1896\n",
      "Epoch 32/60, Loss: 0.1889\n",
      "Epoch 33/60, Loss: 0.1847\n",
      "Epoch 34/60, Loss: 0.1870\n",
      "Epoch 35/60, Loss: 0.1815\n",
      "Epoch 36/60, Loss: 0.1813\n",
      "Epoch 37/60, Loss: 0.1812\n",
      "Epoch 38/60, Loss: 0.1794\n",
      "Epoch 39/60, Loss: 0.1803\n",
      "Epoch 40/60, Loss: 0.1788\n",
      "Epoch 41/60, Loss: 0.1795\n",
      "Epoch 42/60, Loss: 0.1753\n",
      "Epoch 43/60, Loss: 0.1770\n",
      "Epoch 44/60, Loss: 0.1706\n",
      "Epoch 45/60, Loss: 0.1703\n",
      "Epoch 46/60, Loss: 0.1697\n",
      "Epoch 47/60, Loss: 0.1688\n",
      "Epoch 48/60, Loss: 0.1661\n",
      "Epoch 49/60, Loss: 0.1667\n",
      "Epoch 50/60, Loss: 0.1653\n",
      "Epoch 51/60, Loss: 0.1631\n",
      "Epoch 52/60, Loss: 0.1620\n",
      "Epoch 53/60, Loss: 0.1615\n",
      "Epoch 54/60, Loss: 0.1601\n",
      "Epoch 55/60, Loss: 0.1588\n",
      "Epoch 56/60, Loss: 0.1593\n",
      "Epoch 57/60, Loss: 0.1552\n",
      "Epoch 58/60, Loss: 0.1547\n",
      "Epoch 59/60, Loss: 0.1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: evaluate message 98d3577a-5c60-4c7e-87f2-7138ea327385\n",
      "\u001b[92mINFO \u001b[0m:      Sent reply\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      Received: reconnect message c1a13cc0-e63c-45e2-bf8c-8093c7829368\n",
      "\u001b[92mINFO \u001b[0m:      Disconnect and shut down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60, Loss: 0.1548\n",
      "[Client 1] Sending model parameters to server.\n",
      "[Client 1] Evaluating model...\n",
      "Validation Loss: 1.0202, Accuracy: 0.7604\n"
     ]
    }
   ],
   "source": [
    "import flwr as fl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Define the same neural network model as on the server\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(15, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train(net, trainloader, epochs=20):\n",
    "    net.train()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output.squeeze(), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Testing function\n",
    "def test(net, valloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_fn = nn.BCELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in valloader:\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output.squeeze(), target)\n",
    "            predicted = (output.squeeze() > 0.5).float()\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "# Define dataset\n",
    "class HeartDiseaseDataset(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.target[idx]\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('client_2.csv')  # Change file name for different clients\n",
    "X = data.drop(columns=[\"TenYearCHD\"])\n",
    "y = data[\"TenYearCHD\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "train_dataset = HeartDiseaseDataset(pd.DataFrame(X_train), pd.Series(y_train))\n",
    "test_dataset = HeartDiseaseDataset(pd.DataFrame(X_test), pd.Series(y_test))\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the Flower Client\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, pid, net, trainloader, valloader):\n",
    "        self.pid = pid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.pid}] Sending model parameters to server.\")\n",
    "        return [val.detach().cpu().numpy() for val in self.net.parameters()]\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        server_round = config.get(\"server_round\", 1)  # Prevent KeyError\n",
    "        local_epochs = config.get(\"local_epochs\", 60)  # Train for 20 epochs before sending updates\n",
    "\n",
    "        print(f\"[Client {self.pid}, round {server_round}] Training for {local_epochs} epochs...\")\n",
    "        \n",
    "        set_parameters(self.net, parameters)  # Load global model parameters\n",
    "        train(self.net, self.trainloader, epochs=local_epochs)  # Train for 20 epochs\n",
    "        return self.get_parameters(config), len(self.trainloader.dataset), {}\n",
    "\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.pid}] Evaluating model...\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return loss, len(self.valloader.dataset), {\"accuracy\": accuracy}\n",
    "\n",
    "# Helper function to set model parameters\n",
    "def set_parameters(net, parameters):\n",
    "    for param, new_param in zip(net.parameters(), parameters):\n",
    "        param.data = torch.tensor(new_param)\n",
    "        \n",
    "def train(net, trainloader, epochs):\n",
    "    net.train()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0005)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for data, target in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = loss_fn(output.squeeze(), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(trainloader):.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = SimpleNN()\n",
    "    client = FlowerClient(pid=1, net=model, trainloader=trainloader, valloader=valloader)\n",
    "\n",
    "    fl.client.start_client(\n",
    "        server_address=\"127.0.0.1:8081\",\n",
    "        client=client.to_client()  # Updated to latest Flower API\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
